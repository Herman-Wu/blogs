<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://herman-wu.github.io/blogs/feed.xml" rel="self" type="application/atom+xml" /><link href="https://herman-wu.github.io/blogs/" rel="alternate" type="text/html" /><updated>2022-10-18T12:27:42-05:00</updated><id>https://herman-wu.github.io/blogs/feed.xml</id><title type="html">Herman’s Notebook</title><subtitle>Here is the repository of notes that I would like to remember and share</subtitle><entry><title type="html">Sync Async Mlops Patterns</title><link href="https://herman-wu.github.io/blogs/2022/10/10/sync-async-mlops-patterns.html" rel="alternate" type="text/html" title="Sync Async Mlops Patterns" /><published>2022-10-10T00:00:00-05:00</published><updated>2022-10-10T00:00:00-05:00</updated><id>https://herman-wu.github.io/blogs/2022/10/10/sync-async-mlops-patterns</id><content type="html" xml:base="https://herman-wu.github.io/blogs/2022/10/10/sync-async-mlops-patterns.html">&lt;h1 id=&quot;sync-async-tasks-pattern-in-mlops-pipeline&quot;&gt;Sync-Async tasks pattern in MLOps pipeline&lt;/h1&gt;

&lt;p&gt;Machine learning operations (MLOps) process needs to &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/ai-machine-learning-mlops#machine-learning-operations-vs-devops&quot;&gt;combine practices in DevOps and machine learning&lt;/a&gt;. In a software project, testing and validating pipelines usually take a few hours or less to run. Most software projects could complete their unit tests in a few hours. Pipeline tasks that run synchronously or in parallel are enough in most cases.
But in a machine learning project, the training and validation steps could take a long time to run, from a few hours to a few days. It is not practical to wait for the training to finish before moving on to the next step. So during the MLOps flow design, we need to take different approaches and find a way to combine synchronous and asynchronous steps in order to run the end-to-end training process efficiently.&lt;/p&gt;

&lt;p&gt;This article will introduce different practices to implement Sync-Async tasks pattern in the MLOps pipeline using the &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/devops/pipelines/get-started/what-is-azure-pipelines?view=azure-devops&quot;&gt;Azure pipeline&lt;/a&gt; and &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines&quot;&gt;Azure Machine Learning (AML) pipeline&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;use-synchronous-task-and-toy-dataset-for-ml-build-validation-and-unit-test&quot;&gt;Use synchronous task and toy dataset for ML build validation and unit test&lt;/h2&gt;

&lt;p&gt;During the build validation phase, we want to validate the code quality quickly and ensure we implement the data processing code and the ML algorithm correctly. The algorithm code should be able to train a model successfully using the provided dataset.
To speed up the process, we can use a small toy dataset to reduce the resource and time required for training the model. We can also reduce the training epoch and parameter range to reduce the training time further.
This approach uses synchronous pipeline tasks for preparing data and running the training. Because ML model training time is limited, the task can wait for the training to finish and then move on to the next step.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-10-10-sync-async-mlops-patterns/ado-aml-sync-task-pattern.png&quot; alt=&quot;synchronous task pattern&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Following is the code snippet that submits an ML training using &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/machine-learning/concept-v2#azure-machine-learning-cli-v2&quot;&gt;AML CLI v2&lt;/a&gt; and waiting for the result in an ADO task.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;amlJobExecutionScript&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;string&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;amlJobSetCommand&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;string&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;AzureCLI@2&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;displayName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Run Azure ML Pipeline and Wait for Results&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; 
    &lt;span class=&quot;na&quot;&gt;azureSubscription&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$(AZURE_RM_SVC_CONNECTION)&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;scriptType&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bash&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;workingDirectory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$(System.DefaultWorkingDirectory)&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;scriptLocation&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;inlineScript&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;inlineScript&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;export AZUREML_CURRENT_CLOUD=&quot;AzureCloud&quot; #Choose a different value according to your cloud environement: AzureCloud, AzureChinaCloud, AzureUSGovernment, AzureGermanCloud&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;run_id=$(az ml job create -f $ \&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;$)&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;echo &quot;RunID is $run_id&quot;&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;if [[ -z &quot;$run_id&quot; ]]&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;then&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;echo &quot;Job creation failed&quot;&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;exit 3&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;fi&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;az ml job show -n $run_id --web&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;status=$(az ml job show -n $run_id --query status -o tsv)&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;if [[ -z &quot;$status&quot; ]]&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;then&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;echo &quot;Status query failed&quot;&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;exit 4&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;fi&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;running=(&quot;NotStarted&quot; &quot;Queued&quot; &quot;Starting&quot; &quot;Preparing&quot; &quot;Running&quot; &quot;Finalizing&quot;)&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;while [[ ${running[*]} =~ $status ]]&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;do&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;sleep 15 &lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;status=$(az ml job show -n $run_id --query status -o tsv)&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;echo $status&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;done&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;if [[ &quot;$status&quot; != &quot;Completed&quot; ]]  &lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;then&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;echo &quot;Training Job failed&quot;&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;exit 3&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;fi&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Other additional parameters:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;AZURE_RM_SVC_CONNECTION&lt;/code&gt; - Azure DevOps service connection name.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;amlJobExecutionScript&lt;/code&gt; - Local path to the YAML file containing the Azure ML job specification.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;amlJobSetCommand&lt;/code&gt; - Additional Azure ML &lt;a href=&quot;https://learn.microsoft.com/en-us/cli/azure/ml/job?view=azure-cli-latest#az-ml-job-create-optional-parameters&quot;&gt;Job parameters&lt;/a&gt;. For example, &lt;code class=&quot;highlighter-rouge&quot;&gt;--name training-object-detection&lt;/code&gt; to specify the job name.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;use-asynchronous-task-for-ml-model-training-step-in-integration-test-pipeline-and-production&quot;&gt;Use asynchronous task for ML model training step in integration test pipeline and production&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-management-and-deployment#mlops-in-machine-learning&quot;&gt;MLOps pipeline usually includes multiple steps&lt;/a&gt;, such as data preprocessing, model training, model evaluation, model registration, and model deployment. Sometimes, we need to run ML training in the integration test and production environments. For example, a defect detection system might want to retrain an ML model using the existing algorithm with a newly updated dataset from a production line. To automate the process, we want to ensure the whole MLOps pipeline can pass the integration test and run correctly in the production environment. However, the model training step could take a long time to finish. We need to use asynchronous tasks to run the model training step and prevent the long waiting time in the main pipeline.
&lt;img src=&quot;/assets/img/2022-10-10-sync-async-mlops-patterns/mlops-ado-aml-async-task.png&quot; alt=&quot;asynchronous task pattern&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In Azure DevOps, the Microsoft-hosted agent has a &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/devops/pipelines/troubleshooting/troubleshooting?view=azure-devops#job-time-out&quot;&gt;job time-out limitation&lt;/a&gt;. You can have a job running for the maximum 360 minutes (6 hours). The pipeline will fail if the model training step is longer than the time limitation. There are a few ways to implement an asynchronous pipeline task in Azure DevOps to prevent the problem.&lt;/p&gt;

&lt;h3 id=&quot;use-azure-pipeline-rest-api-task-to-invoke-published-azure-ml-pipelines-and-wait-for-the-post-back-event&quot;&gt;Use Azure Pipeline REST API task to invoke published Azure ML pipelines and wait for the post-back event&lt;/h3&gt;

&lt;p&gt;In this approach, you &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-deploy-pipelines&quot;&gt;publish your AML pipeline&lt;/a&gt; and get a REST endpoint for the pipeline. And then you can use Azure Pipeline &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/utility/http-rest-api?view=azure-devops&quot;&gt;REST API task&lt;/a&gt; to invoke published Azure ML pipelines and wait for the post-back events. To wait for the post-back event, we need to set the &lt;code class=&quot;highlighter-rouge&quot;&gt;waitForCompletion&lt;/code&gt; attribute of the REST API task to &lt;code class=&quot;highlighter-rouge&quot;&gt;true&lt;/code&gt;.
&lt;img src=&quot;/assets/img/2022-10-10-sync-async-mlops-patterns/ado-aml-async-restapi.png&quot; alt=&quot;Use Azure Pipeline REST API task to invoke published Azure ML pipelines&quot; /&gt;
The &lt;a href=&quot;https://github.com/cse-labs/code-with-mlops/blob/main/docs/guidance-and-examples/azure-ml-tips-and-tricks/azure-ml-from-azdo.md&quot;&gt;Invoking Azure ML Pipeline From Azure DevOps&lt;/a&gt; document in this playbook has more detail implementation introduction.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Limitation: The latest AML CLI/SDK v2 doesn’t support AML pipeline web API yet.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;use-an-aml-component-to-invoke-rest-api-of-another-azure-pipeline&quot;&gt;Use an AML component to invoke REST API of another Azure Pipeline&lt;/h3&gt;

&lt;p&gt;An AML component is a self-contained piece of code that accomplish a task in a machine learning pipeline. It is the building block of am AML pipeline.&lt;/p&gt;

&lt;p&gt;In this implementation, we use Azure ML CLI/SDK v2 to submit the AML pipeline job. And in the final step of pipeline job, use an &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/machine-learning/concept-component&quot;&gt;AML component&lt;/a&gt; to invoke REST API of another &lt;a href=&quot;https://learn.microsoft.com/en-us/rest/api/azure/devops/pipelines/runs/run-pipeline?view=azure-devops-rest-6.0&quot;&gt;Azure Pipeline&lt;/a&gt; to trigger the next steps.&lt;br /&gt;
&lt;img src=&quot;/assets/img/2022-10-10-sync-async-mlops-patterns/mlops-ado-aml-async-components.png&quot; alt=&quot;Use an AML component to invoke REST API of another Azure Pipeline&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Following are the code snippets of an abbreviate reference implementation of the trigger Azure pipeline AML component.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Trigger Azure Pipeline python code : &lt;code class=&quot;highlighter-rouge&quot;&gt;ado-pipeline-trigger.py&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;requests&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;requests.structures&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CaseInsensitiveDict&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;argparse&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;azure.keyvault.secrets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SecretClient&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;azure.identity&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DefaultAzureCredential&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;parse_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Parse input args&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# setup arg parser
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ArgumentParser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# add arguments
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;--modelpath&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;required&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Path to input model&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;--modelname&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;required&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Name of the registered model&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;--kvname&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;required&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Key Vault Resource Name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;--secretname&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;required&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Secret name for ADO Personal Access Token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;--org&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;required&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ADO organization Name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;--project&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;required&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ADO project Name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;--branch&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;required&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ADO repo branch name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;--apiversion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;required&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ADO restful api version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;--pipelineid&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;required&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ID of the pipeline you need to trigger&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;--pipelineversion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;required&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Pipeline version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# parse args
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# return args
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_run_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modelpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Read run_id from MLmodel&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mlmodel_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modelpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;MLmodel&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;run_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mlmodel_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;modelfile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;modelfile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;readline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;run_id&quot;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;run_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;':'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;run_id&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_secret_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kv_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;secret_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Get the secret value from keyvault&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;kv_uri&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://{kv_name}.vault.azure.com&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;credential&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DefaultAzureCredential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SecretClient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vault_url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kv_uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;credential&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;credential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Retrieving ADO personal access token {secret_name} from {kv_name}.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;retrieved_secret&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_secret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;secret_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;retrieved_secret&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;trigger_pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Trigger Azure Pipeline&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;run_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_run_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modelpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;secret_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_secret_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kvname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;secretname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CaseInsensitiveDict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;basic_auth_credentials&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;secret_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Content-Type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;application/json&quot;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;request_body&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;resources&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;&quot;repositories&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;&quot;self&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;s&quot;&gt;&quot;refName&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;branch&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;variables&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;&quot;model_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modelname&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;&quot;run_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;run_id&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;https://dev.azure.com/{}/{}/_apis/pipelines/{}/runs?pipelineVersion={}&amp;amp;api-version={}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;org&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;project&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipelineid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipelineversion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apiversion&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;url: {url}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;resp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;basic_auth_credentials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;request_body&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;response code {resp.status_code}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;resp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;raise_for_status&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# run script
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# parse args
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parse_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# trigger model registration pipeline
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;trigger_pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Trigger Azure Pipeline AML component: &lt;code class=&quot;highlighter-rouge&quot;&gt;component_pipeline_trigger.yaml&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;s&quot;&gt;$schema&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;trigger_pipeline&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;display_name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;trigger Azure pipeline&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;modelpath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mlflow_model&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;modelname&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;string&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kvname&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;string&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;secretname&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;string&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;org&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;string&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;project&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;string&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;branch&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;string&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiversion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;string&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;pipelineid&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;integer&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;pipelineversion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;integer&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;../../../src/pipeline_trigger/&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;environment&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;azureml:sklearn-jsonline-keyvault-env@latest&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;&amp;gt;-&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ython ado-pipeline-trigger.py&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-modelpath ${{inputs.modelpath}}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-modelname ${{inputs.modelname}}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-kvname ${{inputs.kvname}}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-secretname ${{inputs.secretname}}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-org ${{inputs.org}}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-project ${{inputs.project}}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-branch ${{inputs.branch}}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-apiversion ${{inputs.apiversion}}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-pipelineid ${{inputs.pipelineid}}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-pipelineversion ${{inputs.pipelineversion}}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;subscribe-azure-ml-event-grid-events-and-use-a-supported-event-handler-to--trigger-another-azure-pipeline&quot;&gt;Subscribe Azure ML Event Grid events, and use a supported event handler to  trigger another Azure Pipeline&lt;/h3&gt;

&lt;p&gt;Azure Machine Learning manages the entire lifecycle of machine learning process, during the lifecycle AML will &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-event-grid#event-types-for-azure-machine-learning&quot;&gt;publish several status events&lt;/a&gt; in Event Grid, such as a completion of training runs event or a registration and deployment of models event. We can use &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/event-grid/event-handlers#supported-event-handlers&quot;&gt;supported event handler&lt;/a&gt; to subscribe these events and react to them.
&lt;img src=&quot;/assets/img/2022-10-10-sync-async-mlops-patterns/mlops-ado-aml-async-eventgrid.png&quot; alt=&quot;Subscribe Azure ML Event Grid events, and use a supported event handler to  trigger another Azure Pipeline&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here are the supported Azure ML events:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Event type&lt;/th&gt;
      &lt;th&gt;Subject format&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Microsoft.MachineLearningServices.RunCompleted&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;experiments/{ExperimentId}/runs/{RunId}&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Microsoft.MachineLearningServices.ModelRegistered&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;models/{modelName}:{modelVersion}&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Microsoft.MachineLearningServices.ModelDeployed&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;endpoints/{serviceId}&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Microsoft.MachineLearningServices.DatasetDriftDetected&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;datadrift/{data.DataDriftId}/run/{data.RunId}&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Microsoft.MachineLearningServices.RunStatusChanged&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;experiments/{ExperimentId}/runs/{RunId}&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;For example, to continue the MLOps pipeline when the ML training is finished, we will subscribe &lt;code class=&quot;highlighter-rouge&quot;&gt;RunCompleted&lt;/code&gt; event and trigger another Azure  Pipeline when the event is published. To trigger another Azure Pipeline, we can use &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/automation/manage-runbooks&quot;&gt;Azure Automation runbooks&lt;/a&gt;,  &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-overview&quot;&gt;Logic Apps&lt;/a&gt;, or &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/azure-functions/functions-overview&quot;&gt;Azure Functions&lt;/a&gt; and implement the trigger next step code in one of them.&lt;/p&gt;

&lt;h2 id=&quot;use-azure-pipeline-self-host-agent-to-run-the-aml-pipeline&quot;&gt;Use Azure Pipeline Self-host agent to run the AML pipeline&lt;/h2&gt;

&lt;p&gt;In this approach, we will use &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/devops/pipelines/agents/agents?view=azure-devops&amp;amp;tabs=browser#install&quot;&gt;Azure Pipeline Self-host agent&lt;/a&gt; to run the AML pipeline . Because there is no  &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/devops/pipelines/troubleshooting/troubleshooting?view=azure-devops#job-time-out&quot;&gt;job time-out limitation&lt;/a&gt; for self-hosted agent, it can trigger an AML training task and wait for the training to finish, then moving on to the next step.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-10-10-sync-async-mlops-patterns/mlops-ado-aml-selfhostagent.png&quot; alt=&quot;Use Azure Pipeline Self-host agent to run the AML pipeline&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This approach uses synchronized tasks. However, we will need to install the agent and maintain the environment that runs the agent.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Some MLOps-related tasks have a different nature compared to DevOps focus tasks. It will impact the tool you could use and the decision of using which tool. This article highlights how the ML training duration affects the task pipeline in the MLOps process. This difference will be more obvious when you include traditional DevOps tasks such as unit testing and integration testing in your MLOps process. This article provides some strategies the operation team could use to mitigate the issue. It also includes code snippets when using Azure Pipeline and Azure Machine Learning Pipeline.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[MLOps for Python with Azure Machine Learning - Azure Architecture Center&lt;/td&gt;
          &lt;td&gt;Microsoft Learn](https://learn.microsoft.com/en-us/azure/architecture/reference-architectures/ai/mlops-python)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[MLOps: Machine learning model management - Azure Machine Learning&lt;/td&gt;
          &lt;td&gt;Microsoft Learn](https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-management-and-deployment)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mercari.github.io/ml-system-design-pattern/&quot;&gt;System design patterns for machine learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Sync-Async tasks pattern in MLOps pipeline</summary></entry><entry><title type="html">Security practices and design principles for implementing a data lakehouse solution in Azure Synapse</title><link href="https://herman-wu.github.io/blogs/datalakehouse/lakehouse/synapse/security/data/sql/serverless%20sql/azure/vnet/private%20endpoint/2022/03/15/Security-practices-and-design-priciples-for-implementing-a-data-lakehouse-solution.html" rel="alternate" type="text/html" title="Security practices and design principles for implementing a data lakehouse solution in Azure Synapse" /><published>2022-03-15T00:00:00-05:00</published><updated>2022-03-15T00:00:00-05:00</updated><id>https://herman-wu.github.io/blogs/datalakehouse/lakehouse/synapse/security/data/sql/serverless%20sql/azure/vnet/private%20endpoint/2022/03/15/Security-practices-and-design-priciples-for-implementing-a-data-lakehouse-solution</id><content type="html" xml:base="https://herman-wu.github.io/blogs/datalakehouse/lakehouse/synapse/security/data/sql/serverless%20sql/azure/vnet/private%20endpoint/2022/03/15/Security-practices-and-design-priciples-for-implementing-a-data-lakehouse-solution.html">&lt;h3 id=&quot;security-practices-and-design-principles-for-implementing-a-data-lakehouse-solution-in-azure-synapse&quot;&gt;&lt;em&gt;Security practices and design principles for implementing a data lakehouse solution in Azure Synapse&lt;/em&gt;&lt;/h3&gt;

&lt;h4 id=&quot;background&quot;&gt;Background&lt;/h4&gt;

&lt;p style=&quot;font-size: 0.9rem;font-style: italic;&quot;&gt;&lt;img style=&quot;display: block;&quot; src=&quot;https://live.staticflickr.com/7618/16591080240_f5b5f86100_b.jpg&quot; alt=&quot;The Lake House &quot; /&gt;&lt;a href=&quot;https://www.flickr.com/photos/80223459@N05/16591080240&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&quot;The Lake House&quot;&lt;/a&gt;&lt;span&gt; by &lt;a href=&quot;https://www.flickr.com/photos/80223459@N05&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;YellowstoneNPS&lt;/a&gt;&lt;/span&gt;. Licensed under &lt;a href=&quot;https://creativecommons.org/publicdomain/mark/1.0/&amp;amp;atype=html&quot; style=&quot;margin-right: 5px;&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;CC PDM 1.0&lt;/a&gt;&lt;a href=&quot;https://creativecommons.org/publicdomain/mark/1.0/&amp;amp;atype=html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; style=&quot;display: inline-block;white-space: none;margin-top: 2px;margin-left: 3px;height: 22px !important;&quot;&gt; &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Synapse is a versatile data platform that supports enterprise data warehousing,  real-time  data analytics, data pipeline,  time-serious data processing, machine learning, and data governance. It integrates several different technologies (e.g., SQL DW, Serverless SQL, Spark, Data Pipeline, Data Explorer, Synapse ML, Purview…) to support these various capabilities. However, this also inevitably increases the complexity of  the system infrastructure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-03-15-Security-practices-and-design-priciples-for-implementing-a-data-lakehouse-solution/synaspe-solution-overview.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2022-03-15-Security-practices-and-design-priciples-for-implementing-a-data-lakehouse-solution/synaspe-solution-overview.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this blog, I would like to share the security learning after implementing a &lt;a href=&quot;https://databricks.com/glossary/data-lakehouse&quot;&gt;data lakehouse&lt;/a&gt; project for an international manufacturing company using Synapse. &lt;a href=&quot;https://databricks.com/glossary/data-lakehouse&quot;&gt;Data Lakehouse&lt;/a&gt;  is a modern data management architecture that combines  data lakes’  cost-efficiency, scale, and flexibility features with data warehouse’s data and transaction management capabilities.  It well supports business intelligence and machine learning (ML) scenarios for many diverse data structures and data sources. Some common use cases for the solution are IoT telemetry analysis, consumer activities and behavior tracking, security log monitoring, or semi-structured data processing.&lt;/p&gt;

&lt;p&gt;We will focus on  the security design and implementation practices  used in the project. In the project, we chose &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/on-demand-workspace-overview&quot;&gt;Synapse serverless SQL&lt;/a&gt;  and &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-overview&quot;&gt;Synapse Spark&lt;/a&gt; to implement  the &lt;a href=&quot;https://blog.starburst.io/part-2-of-current-data-patterns-blog-series-data-lakehouse&quot;&gt;data lakehouse pattern&lt;/a&gt;. Following is the high-level solution design architecture.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://techcommunity.microsoft.com/t5/image/serverpage/image-id/329206iAD1B6F56B8C16E88/image-size/large?v=v2&amp;amp;px=999&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;
&lt;figcaption align=&quot;center&quot;&gt;Fig.1   High-level concept of the solution  &lt;br /&gt; 
Source: &lt;a href=&quot;https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/the-best-practices-for-organizing-synapse-workspaces-and/ba-p/3002506&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;The best practices for organizing Synapse workspaces and lakehouse&lt;/a&gt; &lt;/figcaption&gt;

&lt;h4 id=&quot;design-focus&quot;&gt;Design Focus&lt;/h4&gt;

&lt;p&gt;We started the security design by using the &lt;a href=&quot;https://www.microsoft.com/en-us/securityengineering/sdl/threatmodeling&quot;&gt;Threat Modeling tool&lt;/a&gt;. The tool helps us communicate with project stakeholders about the potential risks and define the trust boundary in the system. Based on the thread modeling result,  &lt;u&gt;Identity and Access control&lt;/u&gt;, &lt;u&gt;Network protection&lt;/u&gt;, and  &lt;u&gt;DevOps security&lt;/u&gt; are prioritized  in the project.  Based on these priorities, we implemented additional security features and changed the infrastructures so we could protect the system and mitigate key security risks identified. These also map to &lt;strong&gt;Access Control&lt;/strong&gt;, &lt;strong&gt;Asset Protection&lt;/strong&gt;, and &lt;strong&gt;Innovation Security&lt;/strong&gt; in the  &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/secure/#security-disciplines&quot;&gt;Cloud Adoption Framework (CAF)’s security disciplines&lt;/a&gt;. We will walk through the design principles and related technologies in more detail  In the following sections.&lt;/p&gt;

&lt;h4 id=&quot;security-design-principles-and-learning&quot;&gt;Security Design Principles and Learning&lt;/h4&gt;

&lt;h5 id=&quot;network-and-asset-protection-design&quot;&gt;&lt;em&gt;Network and Asset protection Design&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;One of the key security assurances principles in the Cloud Adoption Framework is the &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/secure/#guiding-principles&quot;&gt;Zero Trust principle&lt;/a&gt;. When designing security for any component or system, we should reduce the risk of attackers expanding access by assuming other resources in the organization are compromised.&lt;/p&gt;

&lt;p&gt;Based on the threat modeling discussion result, we follow the &lt;a href=&quot;https://docs.microsoft.com/en-us/security/zero-trust/deploy/networks#i-network-segmentation-many-ingressegress-cloud-micro-perimeters-with-some-micro-segmentation&quot;&gt;micro-segmentation deployment&lt;/a&gt; recommendation in zero-trust and define several &lt;a href=&quot;https://insights.sei.cmu.edu/blog/cybersecurity-architecture-part-2-system-boundary-and-boundary-protection/&quot;&gt;security boundaries&lt;/a&gt;. VNet and &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/synapse-analytics/security/workspace-data-exfiltration-protection#:~:text=Azure%20Synapse%20Analytics%20workspaces%20support%20enabling%20data%20exfiltration,data%20to%20locations%20outside%20of%20your%20organization%E2%80%99s%20scope.&quot;&gt;Synapse data exfiltration protection&lt;/a&gt; are the key technologies used to implement the security boundary and protect the system’s data assets and critical components.&lt;/p&gt;

&lt;p&gt;Considering Synapse is a composition of &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/synapse-analytics/overview-what-is&quot;&gt;several different technologies&lt;/a&gt;, we need to :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;u&gt;Identify essential  components of Synapse and related services used in the project&lt;/u&gt;.&lt;/p&gt;

    &lt;p&gt;Synapse is a very versatile data platform. It can handle and fulfill many different data processing needs. First, we need to decide which components in Synapse are used in the project to plan how to protect them. Also, we need to determine what other services are communicating with these Synapse’s components. In the data data lakehouse architecture, &lt;em&gt;Synapse Serverless SQL, Synapse Spark, Synpase Pipeline,Azure Data Lakes and Azure DevOps&lt;/em&gt; are the key components.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;u&gt;Define the __legal communication behaviors__ between the components&lt;/u&gt;.&lt;/p&gt;

    &lt;p&gt;We need to define the “legal” communication behaviors between the components. For example, do we want the Synapse Spark engine to communicate with the dedicated SQL instance directly, or do we want the spark engine to communicate with the database through a proxy such as Synapse Data Integration pipeline or Data Lake?&lt;/p&gt;

    &lt;p&gt;Based on the Zero trust principle, we should block the communication if there is no business need for the interaction. For example, we should block the communication if a Synapse Spark engine directly communicates with Data Lake storage in an unknown tenant.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;u&gt;Chose the proper security solution that can enforce the defined communication behaviors&lt;/u&gt;.&lt;/p&gt;

    &lt;p&gt;In Azure, several security technologies are capable of enforcing the defined service communication behaviors. For example, in Azure Data Lake storage, you can use a white-list IP address to control its access, but you can also choose allowed VNet, Azure services, or resource instances. Each protection method provides different security protection and needs to be selected based on the business needs and environmental limitations. I will describe the configuration we used in our project in the next section.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;u&gt;Add threat detection and advanced defense for critical resources&lt;/u&gt;.&lt;/p&gt;

    &lt;p&gt;For critical resources, it is better to add threat detection and advanced defense. These services help identify threats and triggers alerts. So the system can notify users about the security breach.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;network-and-asset-protection-implementation-in-the-project&quot;&gt;&lt;em&gt;Network and Asset protection Implementation in the project&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;In the data lakehouse solution, we designed and controlled the service’s interaction behaviors based on business requirements to mitigate security threats. The following table shows the defined communication behaviors and security solutions used in the project.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;From (Client)&lt;/th&gt;
      &lt;th&gt;To (Service)&lt;/th&gt;
      &lt;th&gt;Behavior&lt;/th&gt;
      &lt;th&gt;Configuration&lt;/th&gt;
      &lt;th&gt;Notes&lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Internet&lt;/td&gt;
      &lt;td&gt;Azure DataLake&lt;/td&gt;
      &lt;td&gt;Deny All&lt;/td&gt;
      &lt;td&gt;Firewall Rule - Default Deny&lt;/td&gt;
      &lt;td&gt;Default: ‘Deny’&lt;/td&gt;
      &lt;td&gt;Firewall Rule - Default Deny&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Synapse Pipeline/Spark&lt;/td&gt;
      &lt;td&gt;Azure DataLake&lt;/td&gt;
      &lt;td&gt;Allow (Instance)&lt;/td&gt;
      &lt;td&gt;VNet - Managed Private EndPoint (Azure DataLake)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Synapse SQL&lt;/td&gt;
      &lt;td&gt;Azure DataLake&lt;/td&gt;
      &lt;td&gt;Allow (Instance)&lt;/td&gt;
      &lt;td&gt;Firewall Rule - Resource instances (Synapse SQL)&lt;/td&gt;
      &lt;td&gt;Synapse SQL needs to access Azure DataLake using Managed Identity&lt;/td&gt;
      &lt;td&gt;N/A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Azure Pipeline Agent&lt;/td&gt;
      &lt;td&gt;Azure DataLake&lt;/td&gt;
      &lt;td&gt;Allow (Instance)&lt;/td&gt;
      &lt;td&gt;* Firewall Rule - Selected Virtual networks &lt;br /&gt; * Service Endpoint - Storage&lt;/td&gt;
      &lt;td&gt;For Integration Testing &lt;br /&gt; bypass: ‘AzureServices’ (firewall rule)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Internet&lt;/td&gt;
      &lt;td&gt;Synapse Workspace&lt;/td&gt;
      &lt;td&gt;Deny All&lt;/td&gt;
      &lt;td&gt;Firewall Rule&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Firewall Rule&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Azure Pipeline Agent&lt;/td&gt;
      &lt;td&gt;Synapse Workspace&lt;/td&gt;
      &lt;td&gt;Allow (Instance)&lt;/td&gt;
      &lt;td&gt;VNet - Private EndPoint&lt;/td&gt;
      &lt;td&gt;Requires 3 Private EndPoints (Dev, Serverless SQL, and Dedicate SQL)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Synapse Managed VNet&lt;/td&gt;
      &lt;td&gt;Internet/ Unauthorized Azure Tenant&lt;/td&gt;
      &lt;td&gt;Deny All&lt;/td&gt;
      &lt;td&gt;VNet - Synapse Data Exfiltration Protection&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Synaspe Pipeline/Spark&lt;/td&gt;
      &lt;td&gt;KeyVault&lt;/td&gt;
      &lt;td&gt;Allow (Instance)&lt;/td&gt;
      &lt;td&gt;VNet - Managed Private EndPoint (KeyVault)&lt;/td&gt;
      &lt;td&gt;Default: ‘Deny’&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Azure Pipeline Agent&lt;/td&gt;
      &lt;td&gt;KeyVault&lt;/td&gt;
      &lt;td&gt;Allow (Instance)&lt;/td&gt;
      &lt;td&gt;* Firewall Rule - Selected Virtual networks &lt;br /&gt; * Service Endpoint - KeyVault&lt;/td&gt;
      &lt;td&gt;bypass: ‘AzureServices’ (firewall rule)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Azure Functions&lt;/td&gt;
      &lt;td&gt;Synapse Serverless SQL&lt;/td&gt;
      &lt;td&gt;Allow (Instance)&lt;/td&gt;
      &lt;td&gt;VNet - Private EndPoint (Synapse Serverless SQL)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Synaspe Pipeline/Spark&lt;/td&gt;
      &lt;td&gt;Azure Monitor&lt;/td&gt;
      &lt;td&gt;Allow (Instance)&lt;/td&gt;
      &lt;td&gt;VNet - Private EndPoint (Azure Monitor)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The below diagram shows the architecture with the network and asset protection design.&lt;br /&gt;
&lt;img src=&quot;/assets/img/2022-03-15-Security-practices-and-design-priciples-for-implementing-a-data-lakehouse-solution/security-architecture01.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2022-03-15-Security-practices-and-design-priciples-for-implementing-a-data-lakehouse-solution/security-architecture01.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For example, the above diagram includes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Create a Synapse workspace with a managed virtual network.&lt;/li&gt;
  &lt;li&gt;Securing data egress from Synapse workspaces through &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/synapse-analytics/security/workspace-data-exfiltration-protection&quot;&gt;Synapse workspaces Data exfiltration protection.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Manage the list of approved Azure AD tenants for the Synapse workspace.&lt;/li&gt;
  &lt;li&gt;Configure network rules to grant only traffic from selected virtual networks access to storage account and disable public network access.&lt;/li&gt;
  &lt;li&gt;Use &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/synapse-analytics/security/synapse-workspace-managed-private-endpoints&quot;&gt;Managed Private Endpoints&lt;/a&gt; to connect Synapse managed VNet with Data Lake.&lt;/li&gt;
  &lt;li&gt;Use &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/storage/common/storage-network-security?tabs=azure-portal#grant-access-from-azure-resource-instances-preview&quot;&gt;Resource Instance&lt;/a&gt; to securely connect Synapse SQL with Data Lake&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For better Network and Asset protection, the following are additional security design considerations.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Deploy Perimeter Networks for Security Zones for Data Pipeline.&lt;/p&gt;

    &lt;p&gt;Because in a data pipeline, data could be loaded from external data sources. When a data pipeline workload requires access to external data and data landing zone, it is better to implement a perimeter network and separate it with a regular ETL pipeline.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Enable &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/storage/common/azure-defender-storage-configure&quot;&gt;Azure Defender for all storage accounts&lt;/a&gt;.&lt;/p&gt;

    &lt;p&gt;Azure Defender provides an additional layer of security intelligence that detects unusual and potentially harmful attempts to access or exploit storage accounts. Security alerts are triggered in Azure Security Center.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/storage/common/lock-account-resource&quot;&gt;Lock storage account&lt;/a&gt; to prevent malicious deletion or configuration changes&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;identity-and-access-control&quot;&gt;&lt;em&gt;Identity and Access control&lt;/em&gt;&lt;/h5&gt;

&lt;p&gt;There are several parts in the system. Each part requires a different Identity and Access Management (IAM) configuration. They will need to collaborate tightly to provide a streamlined user experience. Therefore, we need to plan the following parts when we implement authentication and authorization control.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-03-15-Security-practices-and-design-priciples-for-implementing-a-data-lakehouse-solution/synapse-access-control.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2022-03-15-Security-practices-and-design-priciples-for-implementing-a-data-lakehouse-solution/synapse-access-control.png&quot; alt=&quot;img&quot; /&gt;
&lt;img src=&quot;img/synapse-access-control.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;u&gt;Chose Identity type in different Access Control Layers&lt;/u&gt;&lt;/p&gt;

    &lt;p&gt;There are four different identity solutions in the system.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;SQL Account (SQL Server)&lt;/li&gt;
      &lt;li&gt;Service Principal (Azure AD)&lt;/li&gt;
      &lt;li&gt;Managed Identity (Azure AD)&lt;/li&gt;
      &lt;li&gt;User AAD Account (Azure AD)&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Also, there are four different access control layers in the system.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Application access layer&lt;/li&gt;
      &lt;li&gt;Synapse access layer&lt;/li&gt;
      &lt;li&gt;SQL DB access layer&lt;/li&gt;
      &lt;li&gt;Azure Data Lake access layer&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;A key part of identity and access control is choosing the right identity solution for different user roles in each access control layer. The &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/architecture/framework/security/security-principles&quot;&gt;Well Architecture Security Design Principles&lt;/a&gt; suggests using native controls and driving simplicity. Therefore, we decided to use User AAD account in the Application, Synapse, and SQL DB access layer to leverage the native first-party IAM solutions while using Managed Identity of Synapse to access Azure Data Lake to simply the authorization process.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;u&gt;Consider Least-privileged access&lt;/u&gt;&lt;/p&gt;

    &lt;p&gt;In Zero trust guiding principles, it suggests providing just-in-time and just-enough-access to critical resources. Azure &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/active-directory/privileged-identity-management/pim-configure&quot;&gt;AD Privileged Identity Management (PIM)&lt;/a&gt; and enhance security deployment check in the future.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;u&gt;Protect Linked Service&lt;/u&gt;&lt;/p&gt;

    &lt;p&gt;Linked services define the connection information needed for the service to connect to external resources. It is important to secure the linked Service configuration and access in Synapse.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Create &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/synapse-analytics/data-integration/linked-service#:~:text=In%20Azure%20Synapse%20Analytics%2C%20a%20linked%20service%20is,Manage%20tab.%20Under%20External%20connections%2C%20select%20Linked%20services.&quot;&gt;Azure Data Lake linked service with Private Links&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Use &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview&quot;&gt;Managed Identity&lt;/a&gt; as the authentication method in linked services&lt;/li&gt;
      &lt;li&gt;Use Azure Key Vault as the secret stored in Linked Service.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;devops-security&quot;&gt;DevOps security&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;u&gt;Use VNet enabled self-hosted pipeline agent&lt;/u&gt;&lt;/p&gt;

    &lt;p&gt;Default Azure DevOp pipeline agent couldn’t support VNet communication because it used a very wide IP range. Therefore, we implemented Azure DevOps &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/devops/pipelines/agents/v2-linux?view=azure-devops&quot;&gt;self-hosted agent&lt;/a&gt; in VNet so the DevOps process can be protected and smoothly communicate with the whole system. In addition, &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/overview&quot;&gt;VM scale sets&lt;/a&gt; are used to ensure the DevOps engine can scale up and down.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2022-03-15-Security-practices-and-design-priciples-for-implementing-a-data-lakehouse-solution/devop-security-archi.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2022-03-15-Security-practices-and-design-priciples-for-implementing-a-data-lakehouse-solution/devop-security-archi.png&quot; alt=&quot;img&quot; /&gt;
&lt;img src=&quot;img/synapse-access-control.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;img/devop-security-archi.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;u&gt;Implement infrastructure security scanning &amp;amp; security smoke testing in CI/CD pipeline&lt;/u&gt;&lt;/p&gt;

    &lt;p&gt;Static analysis tool for scanning infrastructure as code (IaC) files can help detect and prevent misconfigurations that may lead to security or compliance problems. In addition, security smoke testing ensures that the vital system security measure is successfully enabled and prevents security risk due to a design fault in the deployment pipeline.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Use static analysis tool for scanning infrastructure as code (IaC) templates to detect and prevent misconfigurations that may lead to security or compliance problems. Use tools such as &lt;a href=&quot;https://www.checkov.io/&quot;&gt;Checkov&lt;/a&gt; or &lt;a href=&quot;https://github.com/accurics/terrascan&quot;&gt;Terrascan&lt;/a&gt; to detect and prevent security risks.&lt;/li&gt;
      &lt;li&gt;Make sure the CD pipeline correctly handles the failure of the deployment. Any deployment failure related to security features should be treated as a critical failure.  It should retry the failed action or hold the deployment.&lt;/li&gt;
      &lt;li&gt;Validate the security measures in the deployment pipeline by running security smoke testing. The security smoke testing, such as validating the configuration status of deployed resources or testing cases that examine critical security scenarios, can ensure that the security design is working as expected.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;security-score-assessment-and-threat-detection&quot;&gt;Security Score assessment and Threat Detection&lt;/h4&gt;

&lt;p&gt;To understand the security status of the system, we used Microsoft Defender for Cloud to assess the infrastructure security and detect the security issues. &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/defender-for-cloud/defender-for-cloud-introduction&quot;&gt;Microsoft Defender for Cloud&lt;/a&gt; is a tool for security posture management and threat protection. It can protect workloads running in Azure, hybrid, and other cloud platforms.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://docs.microsoft.com/en-us/azure/defender-for-cloud/media/defender-for-cloud-introduction/defender-for-cloud-synopsis.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can enable Defender for Cloud’s free plan  on all your current Azure subscriptions when you visit the Defender for Cloud pages in the Azure portal for the first time. I highly recommend enabling  it so you can get your Cloud security posture evaluation and suggestions. And it is all free, so why not ☺️. Microsoft Defender for Cloud will provide you security score and some security hardening guidance for your subscription.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://docs.microsoft.com/en-us/azure/defender-for-cloud/media/secure-score-security-controls/single-secure-score-via-ui.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The information is quite straightforward, and the recommendations are excellent. Many of the recommendations can are easy to take action.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://docs.microsoft.com/en-us/azure/defender-for-cloud/media/secure-score-security-controls/remediate-vulnerabilities-control.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And if you need advanced security management and threat detection capabilities, which provide features such as suspicious activities detection and alerting. You can enable  Cloud workload protection individually for different resources. So you have the option to choose the most cost-effective way to protect the system.&lt;/p&gt;

&lt;h4 id=&quot;summary&quot;&gt;Summary&lt;/h4&gt;

&lt;p&gt;With the combination of Synapse Serverless SQL and Synapse Spark, we built a flexible,  scalable ,and cost-effective data lakehouse solution in the project.This article tried to summarize the security design principle and practices we implemented in the solution. We start from the &lt;a href=&quot;https://www.microsoft.com/en-us/securityengineering/sdl/threatmodeling&quot;&gt;Threat Modeling tool&lt;/a&gt; and guidance in the  &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/secure/#security-disciplines&quot;&gt;Cloud Adoption Framework (CAF)’s security disciplines&lt;/a&gt;. To harden the security protection, the project team decided to focus on Network and Asset Protection, Identity and Access control, and DevOps security. We also evaluated the security score of the system and reviewed the security suggestions provided by Microsoft Defender for cloud. You need to choose between several different configurations when you want to protect the network. The article also describes the design after we compared different options. Identity and Access control are crucial for securing the data asset in the system.  Especially for the data lakehouse solution, you need to map different access control layers and choose the right identity solution. I hope this learning will also help you implement a secure data lakehouse solution on Azure Synapse.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h5 id=&quot;reference&quot;&gt;Reference&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.starburst.io/part-2-of-current-data-patterns-blog-series-data-lakehouse&quot;&gt;Current Data Patterns Blog Series: Data Lakehouse&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/the-data-lakehouse-the-data-warehouse-and-a-modern-data-platform/ba-p/2792337?msclkid=c7eddbcbb24411ecae0f0ec795c2ad28&quot;&gt;The Data Lakehouse, the Data Warehouse and a Modern Data platform architecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/the-best-practices-for-organizing-synapse-workspaces-and/ba-p/3002506&quot;&gt;The best practices for organizing Synapse workspaces and lakehouse&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/defender-for-cloud/secure-score-security-controls#:~:text=Defender%20for%20Cloud%20continually%20assesses,lower%20the%20identified%20risk%20level.&quot;&gt;Secure score in Microsoft Defender for Cloud&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.thedataguy.blog/azure-synapse-understanding-private-endpoints/&quot;&gt;Understanding Azure Synapse Private Endpoints&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dzone.com/articles/azure-synapse-analytics-new-insights-into-data-sec&quot;&gt;Azure Synapse Analytics – New Insights Into Data Security&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/security/benchmark/azure/baselines/synapse-analytics-security-baseline&quot;&gt;Azure security baseline for Azure Synapse dedicated SQL pool (formerly SQL DW)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.fugue.co/blog/cloud-network-security-101-azure-service-endpoints-vs.-private-endpoints&quot;&gt;Cloud Network Security 101: Azure Service Endpoints vs. Private Endpoints&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/synapse-analytics/security/how-to-set-up-access-control&quot;&gt;How to set up access control for your Synapse workspace&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/synapse-analytics/security/synapse-private-link-hubs&quot;&gt;Connect to Azure Synapse Studio using Azure Private Link Hubs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/how-to-deploy-your-synapse-workspace-artifacts-to-a-managed-vnet/ba-p/2764232&quot;&gt;How-To Deploy your Synapse Workspace Artifacts to a Managed VNET Synapse Workspace&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/synapse-analytics/cicd/continuous-integration-delivery&quot;&gt;Continuous integration and delivery for an Azure Synapse Analytics workspace&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Security practices and design principles for implementing a data lakehouse solution in Azure Synapse</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://azurecomcdn.azureedge.net/cvt-ee71595d3667788def73479da1629d673313a0b081e460fc596839b82f34a2df/images/page/services/machine-learning/mlops/steps/mlops-slide1-step3.svg" /><media:content medium="image" url="https://azurecomcdn.azureedge.net/cvt-ee71595d3667788def73479da1629d673313a0b081e460fc596839b82f34a2df/images/page/services/machine-learning/mlops/steps/mlops-slide1-step3.svg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Reflections on my 2020 Data Projects - Part II</title><link href="https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/azure/data%20pipeline/reflections/2021/01/01/Reflection-of-my-2020-Data-Projects-part2.html" rel="alternate" type="text/html" title="Reflections on my 2020 Data Projects  - Part II" /><published>2021-01-01T00:00:00-06:00</published><updated>2021-01-01T00:00:00-06:00</updated><id>https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/azure/data%20pipeline/reflections/2021/01/01/Reflection-of-my-2020-Data-Projects-part2</id><content type="html" xml:base="https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/azure/data%20pipeline/reflections/2021/01/01/Reflection-of-my-2020-Data-Projects-part2.html">&lt;h4 id=&quot;lessons-i-learned-and-want-to-keep-in-mind-in-2021&quot;&gt;&lt;em&gt;Lessons I learned and want to keep in mind in 2021+&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;a href=&quot;https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/azure/data%20pipeline/reflections/2021/01/01/Reflection-of-my-2020-Data-Projects-part1.html&quot;&gt;« Part-I&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;6-every-small-problem-can-be-big&quot;&gt;#6 Every small problem can be BIG&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2021-01-01-Reflection-of-my-2020-Data-Projects-part1/quarantine-4981010_640.jpg&quot; alt=&quot;img&quot; /&gt;
&lt;br /&gt;
Image by &lt;a href=&quot;https://pixabay.com/users/evgenit-4930349/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=4981010&quot;&gt;Evgeni Tcherkasski&lt;/a&gt; from &lt;a href=&quot;https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=4981010&quot;&gt;Pixabay&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;When we were developing solutions for different projects, the dev team usually started by implementing the functional logic that fulfills the design. We used small volumes of data to validate the functionality. During these functional development tasks, there are a few things that deserve our additional attention because they could impact the system’s stability when the system is running in full scale and handles production workload during peak time.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Cloud is a live infrastructure&lt;/strong&gt;: Cloud is an environment that keeps evolving over time. All cloud vendors are working hard to improve existing services and introduce new capabilities in their platform. It also means the services on the cloud will be upgraded every couple of months. Though cloud vendors/operators will try to prevent service interruptions and sometimes reduce the interruption time to less than one second, it still impacts heavy load systems/components that happen to have hundreds of data processing tasks at the moment. Some design pattern like 
&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/architecture/patterns/claim-check&quot;&gt;Claim-Check Pattern&lt;/a&gt; or &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/architecture/patterns/queue-based-load-leveling&quot;&gt;Queue-Based Load Leveling pattern&lt;/a&gt; could help mitigate the impact.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Network Transient Failure&lt;/strong&gt;: Network is the vessel of a modern system. It connects every component and transmits a massive information payload in high frequency. Many services and SDKs already have build-in mechanisms to handle transient network failure. However, for a system with heavy network traffic, we should review how it works in the architecture and consider adding additional mechanisms. 
The most common practice to handle network failure is to resend and retry the network operation. What needs to pay attention is it’s better to implement a “smarter” retry strategy for a system that has a huge workload and heavy network operation. Some libraries like Polly (.net core), Tenacity (python) can help us implement the retry action using an advanced algorithm.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Optimize Disk I/O&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Understand the distributed file storage system underneath your data solution. For example, check Sharding for Mongo DB, Extents for Azure Data Explorer, Partitioning (bucketing) for Databricks Delta table.&lt;/li&gt;
      &lt;li&gt;Check how data indexing and partitioning impacts the disk I/O patterns.&lt;/li&gt;
      &lt;li&gt;Understand the max data access API call for your cloud file storage solution. Remember, reading one file is not an API call; it will require multiple API calls such as directory lookup, check file meta-data, read each chunk of the files.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Log exploding:&lt;/strong&gt;&lt;br /&gt;
  In most systems, there will be trace logs to help system administrators monitor and debug the system’s operation status. Pay attention to how these logs are being triggered and the file size growth speed of logs. In a large system, the log files could increase from KB to multiple GBs in a few hours and drag the system’s performance. If the system uses checkpoint files, logging API calls, they also need to be checked.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;7-end-to-end-testing&quot;&gt;#7 End-to-End Testing&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2021-01-01-Reflection-of-my-2020-Data-Projects-part1/test-4092025_1920.jpg&quot; alt=&quot;img&quot; /&gt;
&lt;br /&gt;
Image by &lt;a href=&quot;https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=4092025&quot;&gt;Gerd Altmann&lt;/a&gt; from &lt;a href=&quot;https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=4092025&quot;&gt;Pixabay&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Testing is a vast topic. It will/should occupy at least nearly half of the developing team’s resources. Since projects always don’t have enough time to do perfect testing, we can only try to spend our limited time on the parts that most likely go wrong and is critical to the system. &lt;em&gt;In data project, we realized the traditional unit test and system integration are not enough to verify the solution quality&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In our projects, dev teams need to implement the code logic that retrieves, move, clean up,  process, and store data. Traditional software practices focus on unit testing and integration testing. They can verify if the code logic is correctly and thoughtfully implemented. But to ensure each developed components meet go-production quality, besides ensuring the code is implemented right, another equally important part is to &lt;strong&gt;test and make sure the infrastructure is also implemented right&lt;/strong&gt;. For example, we want to check:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If the infrastructure can scale adequately to handle different data volume&lt;/li&gt;
  &lt;li&gt;If the computing resources are efficiently utilized for diverse workload&lt;/li&gt;
  &lt;li&gt;If all the tasks can be finished after peak hours&lt;/li&gt;
  &lt;li&gt;If we calculate the required system resource and make projections in the right way&lt;/li&gt;
  &lt;li&gt;If the system boundary/limitation is what we understand&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To know the answer to these questions, We adopted a few techniques:&lt;/p&gt;

&lt;h4 id=&quot;mimic-real-world-and-edge-case-workload&quot;&gt;&lt;u&gt;Mimic Real World and Edge Case Workload&lt;/u&gt;:&lt;/h4&gt;
&lt;p&gt;The simplest way is to deploy the whole environment and run end-to-end testing using workloads that mimic the production scenario or scenario we want to test. Unlike website load testing, this part usually takes efforts to customize and can be a small standalone project on its own. To build the workload simulation solution, Kubernetes is a popular choice. It’s flexible and cost-effective.&lt;/p&gt;

&lt;h4 id=&quot;leverage-gherkin-language-and-bdd-tools&quot;&gt;&lt;u&gt;Leverage Gherkin language and BDD Tools&lt;/u&gt;:&lt;/h4&gt;

&lt;p&gt;“&lt;em&gt;Behavior-driven development (or BDD) is an agile software development technique that encourages collaboration between developers, QA and non-technical or business participants in a software project&lt;/em&gt;” - &lt;a href=&quot;https://behave.readthedocs.io/en/stable/philosophy.html&quot;&gt;behave doc&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We want to know how the system performs in different workloads and found it easier to describe the workload criteria and expected results in a structured pattern. BDD tools like &lt;a href=&quot;https://en.wikipedia.org/wiki/Cucumber_(software)&quot;&gt;Cucumber&lt;/a&gt;, &lt;a href=&quot;https://behave.readthedocs.io&quot;&gt;behave&lt;/a&gt;, &lt;a href=&quot;https://docs.behat.org/en/latest/&quot;&gt;behat&lt;/a&gt; and their script language &lt;a href=&quot;https://behave.readthedocs.io/en/stable/philosophy.html#the-gherkin-language&quot;&gt;Gherkin&lt;/a&gt; (a business Readable language for behavior descriptions) match such requirements. Business users can easily change the workload patterns and automatically run the test to verify if they meet the performance requirements.&lt;/p&gt;

&lt;h4 id=&quot;provision-load-testing-environment-using-cd-pipeline-parameters-&quot;&gt;&lt;u&gt;Provision Load Testing Environment using CD pipeline parameters &lt;/u&gt;:&lt;/h4&gt;
&lt;p&gt;The other practice is integrating the load testing environment into the CI/CD pipeline and using a parameter to control the kind of environment the CD pipeline will provide. Due to the tight integration with infrastructure and relying on some cloud PASS services, the CI/CD pipeline will become more complicated and requires careful planning. Plan and Integrate the provision of load testing environments in the CD pipeline can reduce the CD pipeline number and make it easier to keep the environment consistent.&lt;/p&gt;

&lt;h4 id=&quot;centralize-performance-metrics-and-perpare-dashboard&quot;&gt;&lt;u&gt;Centralize Performance Metrics and Perpare Dashboard&lt;/u&gt;:&lt;/h4&gt;
&lt;p&gt;Since there are several components in the system, each part has different performance metrics, with an entirely different measuring unit and business meaning. It’s easier to have a centralized place to query and view all these metrics.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2021-01-01-Reflection-of-my-2020-Data-Projects-part1/dashboard-all.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;8-measure-your-data-drift&quot;&gt;#8 Measure your Data Drift&lt;/h3&gt;

&lt;p&gt;Data Drift is a big issue in machine learning systems. It means the profile and distribution of the data changed, and the previous trained/tested model might not be able to predict the result on the new data with the same accuracy. But not only impacts machine learning prediction, data drift will also affect the data processing pipeline.&lt;/p&gt;

&lt;p&gt;When we design and test the data processing pipeline, it relies on understanding data to optimize the operation. Especially for data aggregation or classification operation, because most data are not evenly distributed, the unbalanced data (skew data) will introduce an uneven workload that will cause low utilization of system resources. We will need to make some adjustments to shuffle data evenly. If data drift, it will break the optimization and cause an even worse unbalanced workload and reduce system efficiency.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2021-01-01-Reflection-of-my-2020-Data-Projects-part1/perf-fine-tune.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Consider adding some metrics to monitor data drift if some components in the system are sensitive and will be impacted by data characteristic change. For example, we watched some Spark tasks’ running time because they are sensitive to data distribution in our scenario. The other common practice is checking the statistical distribution of recent data regularly.&lt;/p&gt;

&lt;h3 id=&quot;9-continuous-learning-with-chaos-engineering&quot;&gt;#9 Continuous learning with Chaos Engineering&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;“Chaos engineering is the discipline of experimenting on a software system in production in order to build confidence in the system‘s capability to withstand turbulent and unexpected conditions”&lt;/em&gt; - &lt;u&gt;[Principles of Chaos Engineering](https://principlesofchaos.org)&lt;/u&gt;&lt;/p&gt;

&lt;p&gt;Chaos Engineering is initially proposed by Netflix in 2010 because they wanted a better way to test and improve their distributed microservice system’s reliability. They found that finding faults in a distributed system goes beyond the capability of standard application testing. Instead of testing the single failure point, chaos engineering aims to generate new knowledge about inter-connections and impacts between components in the system.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://phoenixnap.com/blog/wp-content/uploads/2020/10/how-chaos-engineering-works.jpg&quot; alt=&quot;HowChaosEngineeringWork&quot; /&gt;
&lt;br /&gt;
&lt;em&gt;image credit: &lt;a href=&quot;https://phoenixnap.com/blog/chaos-engineering&quot;&gt;Chaos Engineering: How it Works, Principles, Benefits, &amp;amp; Tools&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Improve system resilience is a continuous learning journey. Though we have mentioned several key learnings, and some of them learned from the production system issues, there are still many parts that can be explored and enhanced. We can systematically learn the system behavior through chaos engineering and understand how it responds to production turbulence.  For a large data project, it is definitely worth including the chaos engineering tools and practices during system development and long-term system operation.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;This post covered topics learned from several data projects I participated  in 2020. I will use it to remind me of some key points that should be considered and prepared for future data projects. Hoping some of them can also benefit your data projects.&lt;/p&gt;

&lt;p&gt;Goodbye 2020 and Welcome 2021.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://databricks.com/session_na20/best-practices-for-building-robust-data-platform-with-apache-spark-and-delta&quot;&gt;Best Practices for Building Robust Data Platform with Apache Spark and Delta&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/analytics-vidhya/big-data-management-governance-bce5f72821c1&quot;&gt;Analytics Challenges — Big Data Management &amp;amp; Governance&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.sqlshack.com/an-overview-of-etl-and-elt-architecture/&quot;&gt;An overview of ETL and ELT architecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.panoply.io/etl-vs-elt-the-difference-is-in-the-how&quot;&gt;ETL Vs ELT: The Difference Is In The How&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=qHykK5pFRW4&quot;&gt;Intro to Chaos Engineering (Video)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://martinfowler.com/articles/evodb.html#AllDatabaseArtifactsAreVersionControlledWithApplicationCode&quot;&gt;Evolutionary Database Design&lt;/a&gt;
-&lt;a href=&quot;https://phoenixnap.com/blog/chaos-engineering&quot;&gt;Chaos Engineering: How it Works, Principles, Benefits, &amp;amp; Tools
&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Lessons I learned and want to keep in mind in 2021+</summary></entry><entry><title type="html">Reflections on my 2020 Data Projects - Part I</title><link href="https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/azure/data%20pipeline/reflections/2021/01/01/Reflection-of-my-2020-Data-Projects-part1.html" rel="alternate" type="text/html" title="Reflections on my 2020 Data Projects  - Part I" /><published>2021-01-01T00:00:00-06:00</published><updated>2021-01-01T00:00:00-06:00</updated><id>https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/azure/data%20pipeline/reflections/2021/01/01/Reflection-of-my-2020-Data-Projects-part1</id><content type="html" xml:base="https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/azure/data%20pipeline/reflections/2021/01/01/Reflection-of-my-2020-Data-Projects-part1.html">&lt;h3 id=&quot;lessons-i-learned-and-want-to-keep-in-mind-in-2021&quot;&gt;&lt;em&gt;Lessons I learned and want to keep in mind in 2021+&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2021-01-01-Reflection-of-my-2020-Data-Projects-part1/new-year-4768119_1280.jpg&quot; alt=&quot;img&quot; /&gt;
Image by &lt;a href=&quot;https://pixabay.com/users/syaibatulhamdi-13452116/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=4768119&quot;&gt;Syaibatul Hamdi&lt;/a&gt; from &lt;a href=&quot;https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=4768119&quot;&gt;Pixabay&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2020 has sunsetted. Looking back over it, I had three projects that needed to process multiple terabytes of data regularly (one of them needs to process hundreds of terabytes of data every day) and one project that wanted to apply good &lt;a href=&quot;https://blogs.nvidia.com/blog/2020/09/03/what-is-mlops/&quot;&gt;MLOps&lt;/a&gt; practices on GitHub. Some projects have gone production and are running on data centers in different continents to serve worldwide customers.&lt;/p&gt;

&lt;p&gt;In this blog post, I want to &lt;strong&gt;summarize the key practices learned from these projects&lt;/strong&gt; which I want to remember and keep in mind in 2021. I want to read them each time before a new data project kicks off and make a better plan.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note! Even for similar workflow, small/medium/large scale projects require different architecture and plan differently. This post focus on large scale data project.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;1-prepare-for-data-schema-change&quot;&gt;#1 &lt;em&gt;Prepare for Data Schema Change&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;This is a hard fact, but &lt;strong&gt;real-world data always evolve and data schema will change overtime&lt;/strong&gt;. Years ago, it was a common practice that we spent a lot of time negotiating the data schema/contract with data providers and tried to define the best schema that can live forever. But we are in a world that data is exponentially growing; more and more data comes from non-traditional OLTP systems. In some projects (especially projects with over several gigabytes of data every day), it’s not realistic to assume we can have a stable data format/schema over time. The requests to support varying data schema keeps pop up in system requirements in my 2020 data projects.&lt;/p&gt;

&lt;p&gt;There are several techniques that can help to handle data schema changes, such as:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/learn/modules/describe-azure-databricks-delta-lake-architecture/2-describe-bronze-silver-gold-architecture&quot;&gt;Bronze, Silver, and Gold architecture&lt;/a&gt;
&lt;br /&gt;
&lt;img src=&quot;https://databricks.com/wp-content/uploads/2019/08/Delta-Lake-Multi-Hop-Architecture-Bronze.png&quot; alt=&quot;Bronze, Silver, and Gold&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Extract,_load,_transform#Common_storage_options&quot;&gt;Extract, load, transform&lt;/a&gt; &lt;br /&gt;
&lt;img src=&quot;https://blog.panoply.io/hubfs/Blog_images/http%20images%20to%20https/etl-processes-described---x----1000-420x---%20(1).jpg&quot; alt=&quot;Extract, load, transform&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Using data format that supports dynamic schema&lt;/strong&gt; such as JSON, Parquet, and  &lt;strong&gt;leveraging capabilities of NoSQL solutions&lt;/strong&gt; such as MongoDB, COSMOS DB, Elastic Search, Azure Data Explorer, Delta Lake, Big Query, DynamoDB…&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Schema versioning&lt;/strong&gt;. You can adopt some techniques like &lt;a href=&quot;https://martinfowler.com/eaaDev/timeNarrative.html&quot;&gt;Temporal Patterns&lt;/a&gt;, &lt;a href=&quot;https://martinfowler.com/articles/evodb.html#AllDatabaseArtifactsAreVersionControlledWithApplicationCode&quot;&gt;Artifacts are version controlled with application code&lt;/a&gt;, &lt;a href=&quot;https://docs.microsoft.com/en-us/archive/msdn-magazine/2009/april/design-patterns-for-data-persistence&quot;&gt;Data Mapper&lt;/a&gt;, &lt;a href=&quot;https://www.mongodb.com/blog/post/building-with-patterns-the-document-versioning-pattern&quot;&gt;Document Versioning Pattern&lt;/a&gt; to implement data versioning.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Most of the techniques share similar principles and can complement each other in handling data schema change&lt;/em&gt;. The key point is paying some extra attention to it. If someone assumes that they will have a stable schema, it’s better to discuss more on this assumption and validate again. The requirement to handle schema change will impact the foundation of the whole system design.&lt;/p&gt;

&lt;h3 id=&quot;2-design-for-query--charting&quot;&gt;#2 &lt;em&gt;Design for Query &amp;amp; Charting&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2021-01-01-Reflection-of-my-2020-Data-Projects-part1/financial-2860753_1280.jpg&quot; alt=&quot;img&quot; /&gt; &lt;br /&gt;
Image by &lt;a href=&quot;https://pixabay.com/users/6689062-6689062/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=2860753&quot;&gt;David Schwarzenberg&lt;/a&gt; from &lt;a href=&quot;https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=2860753&quot;&gt;Pixabay&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;A good data system should at least fulfill the team’s basic query and charting requirements&lt;/em&gt;. It is very common that  &lt;strong&gt;data collectors&lt;/strong&gt; (such as data engineers, people who collect and ingest the data) and &lt;strong&gt;data consumers&lt;/strong&gt; (BI reporter, data analyzer, ML researcher.. ) &lt;em&gt;are from different teams, and they don’t understand each others’ languages in projects&lt;/em&gt;.  Considering the skill sets required for different roles and different groups also have different business goals, this is very normal. However, bringing the promised benefits of the whole data system requires deep engagement from both parties. A couple of times, we saw that technologies are chosen and built based on one side’s requirements, but the solution poorly meets the other side’s needs.&lt;/p&gt;

&lt;p&gt;It is usually easier to figure out the data collector requirements than data consumers’ needs because data collectors’ requirements are mostly related to the &lt;a href=&quot;https://www.zdnet.com/article/volume-velocity-and-variety-understanding-the-three-vs-of-big-data/&quot;&gt;3Vs of Big Data&lt;/a&gt; - &lt;strong&gt;Volume, Variant, and Velocity&lt;/strong&gt;.  And we can quantify them with specific numbers in the requirements specification. The data consumers’ requirements often depend on the business context, problem domain, and size, skill set of the data consumers team; it requires some more digging to figure out.&lt;/p&gt;

&lt;p&gt;Here are some common factors to consider:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Visualization Tools&lt;/strong&gt;:  Some teams may have strong Excel skills; they are fluent in using PowerPivot and Excel functions to generate in-depth interactive reports. Some teams like the advanced graphical presentation provided by dashboard/reporting tools such as PowerBI, Tableau. Some groups like the powerful programmability in Jupyter Notebook or Shinny. Also, most of the data platforms will provide their build-in tools to visualize data. For example, Azure Data Explore provides very handy and powerful charting capabilities for historical log data and time-series data analysis.&lt;br /&gt;
&lt;img src=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/media/time-series-analysis/time-series-operations.png&quot; alt=&quot;AzureDataExplorer-Chart&quot; /&gt;
&lt;br /&gt;
&lt;em&gt;Residual time series chart in Azure Data Explorer&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data Post Processing/Query Tools and Languages&lt;/strong&gt;: Data consumers will need to further process data to fulfill their analytic requirements. Languages like SQL, R, and Python (ok, you can also add Julia) are very common and powerful ones that data analyzers use to further “&lt;a href=&quot;https://stackoverflow.com/questions/577892/what-does-data-massage-mean&quot;&gt;&lt;strong&gt;massage data&lt;/strong&gt;&lt;/a&gt;”. But each analysis domain has it’s own unique data processing pattern. Based on different design purposes,  &lt;em&gt;different data platforms also provides their own data query syntax and query engine to support and simplify data post-processing for specific domains&lt;/em&gt;. Understanding each platform’s strength and choosing the best-fit one is critical to the project’s success. For example, it’s easy to implement complex text filtering and parsing in Elastic Search, but it’s hard to implement complex data join and aggregation in it. The other example is Azure Data Explorer, which focuses on historical log data analytics,  provides powerful &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/kusto/concepts/&quot;&gt;Kusto Query Lanaguage&lt;/a&gt; to simply log parsing, aggregation, data join, and semi-structure data processing. It is one of my favorite query languages.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Performance goal and Limitation to retrieve insights from “Big Data”&lt;/strong&gt;: Processing large volumes of data is not easy; different solution applies different strategies to achieve their performance goal. They use different index algorithms, in-memory computing, cluster resource allocation, file storage structure, data compression type, etc., to archive their performance goal. These strategies will also come with some limitations like “limitation of concurrent users”, “unbalance computing resource allocation”, “required huge memory caches”, “data paging support”…etc. Review the pro and cons of different solutions and test them in the project’s main data query scenarios.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both data collectors and data consumers are critical stakeholders of a data project. In the solution design phase, it’s helpful to have deep engagement with both two teams. Spending a little more time on how the data query pattern looks like and considering the data analysis tools suitable for users, you will have a bright outlook for the system.&lt;/p&gt;

&lt;h3 id=&quot;3-balance-the-workload-of-each-resources&quot;&gt;#3 Balance the workload of each resources&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://www.denofgeek.com/wp-content/uploads/2017/04/obi_wan.jpg&quot; alt=&quot;startwar-balance&quot; /&gt;
&lt;br /&gt;
&lt;em&gt;image credit: &lt;a href=&quot;https://www.denofgeek.com/movies/star-wars-the-last-jedi-and-the-balance-of-the-force/&quot;&gt;Rob Leane -Den of Geek UK&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;When we worked on small or medium-sized data projects, we implemented the solution using one or two key technologies such as Databricks or Azure Data Explorer. These technologies come with some default capabilities to ingest, manage, and query data. We focused on providing proper configuration on these platforms.&lt;/p&gt;

&lt;p&gt;However, when we worked on large scale projects, instead of relying on built-in capabilities,  we found we need to &lt;strong&gt;re-architect the system to further distribute the computation workloads to different components in the system&lt;/strong&gt;. We need to spend more time thinking about how to &lt;strong&gt;make the data more “swallowable” for the data platform&lt;/strong&gt;. “Swallowable” could mean things like reducing the data fragmentation, easier to build index, remove dirty data, remove duplicated data… etc.&lt;/p&gt;

&lt;p&gt;The workload distribution process is also a process to find the balance of each component. Here are some points that might help:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Size of your batch and how it impacts latency and memory consumption&lt;/li&gt;
  &lt;li&gt;Choose different data formats, consider it’s the impact to data serialize/un-serialize time and disk I/O&lt;/li&gt;
  &lt;li&gt;Decide where to filter, split, aggregate data so the next component can reduce workload and simplify management and configuration complexity.&lt;/li&gt;
  &lt;li&gt;It is also related to the maximum scale-out/in capabilities of each component. All the data input and output velocity of these components can then be within its connected components’ bandwidth.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For a large scale project, the right solution will be the tasks are well distributed among all critical components, and the workload is balanced.  So the designed system can have room growth its capacity, handle peak-time traffic, and prepare for future business growth.&lt;/p&gt;

&lt;h3 id=&quot;4-wow-there-are-duplicated-data&quot;&gt;#4 Wow, there are duplicated data&lt;/h3&gt;

&lt;p&gt;This is another common issue when we process a large amount of data. We could have duplicated data for dozens of reasons. It could be the data source sent twice because of a network issue; it could be some parts of the data processing pipeline partially failed and the system is trying to resume from the latest checkpoint; it could be the underneath message service only guarantee at-least-once, etc.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2021-01-01-Reflection-of-my-2020-Data-Projects-part1/window-941625_1280.jpg&quot; alt=&quot;img&quot; /&gt;
&lt;br /&gt;
Image by &lt;a href=&quot;https://pixabay.com/users/martinharry-1411929/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=941625&quot;&gt;Martin Pyško&lt;/a&gt; from &lt;a href=&quot;https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=941625&quot;&gt;Pixabay&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We need to evaluate the business impact and the cost we want to pay for mitigating the issue. Common strategies to handle duplicated data are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Ingestion Time Check&lt;/strong&gt;: We can have ingestion time check by adding data processing logs or checkpoint files. Then all the ingestion operations need to verify the log/checkpoint file before process the data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Settling Basin Filter&lt;/strong&gt;: In this strategy, we store data in a temporary storage/table. Then, we compared the data with recently ingested data to make sure they are not duplicated data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Query Time Check&lt;/strong&gt;: In some systems, a few duplicated data can be ignored in most use cases (e.g., website user login log for analyzing user demographic/ geographical distribution), and only a few of its use cases require duplicated data check. For such a scenario, we can check duplicated data at query time for only required use cases; then we can reduce the system cost for de-duplicated data check.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Under Some Threshold, Ignore It&lt;/strong&gt;: It’s the best solution if the business context allows it. De-duplication operations usually occupy a certain amount of system resources and impact system performance. In a large data processing system, it’s very costly to implement it. It will be helpful if we can ignore it when it doesn’t impact the business goad.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Besides the above strategies to handle duplicated data, one other point is understanding where the duplicated data comes from. Which part of the system or infrastructure services provides at-least-once and which part can support exactly-once. If the system use checkpoint to resume the failed operation, try to understand how it works.&lt;/p&gt;

&lt;h3 id=&quot;5-understanding-of-core-technologies&quot;&gt;#5 Understanding of Core Technologies&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2021-01-01-Reflection-of-my-2020-Data-Projects-part1/book-4126483_1280.jpg&quot; alt=&quot;img&quot; /&gt;
&lt;br /&gt;
Image by &lt;a href=&quot;https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=4126483&quot;&gt;Gerd Altmann&lt;/a&gt; from &lt;a href=&quot;https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=4126483&quot;&gt;Pixabay&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This point should be needless to say, but I add it here not just because it’s fundamental but also it could take consider mount of time to understand the design philosophy and available configuration parameters of the different data platforms. Many settings could impact each other; we should keep in mind that planning enough time to learn, test, and verify the best configuration setting is required.&lt;/p&gt;

&lt;p&gt;For example, in the core technologies I used in these projects, here are some key configurations that need to check, calculate, and test:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Databricks&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Number of Cores&lt;/li&gt;
      &lt;li&gt;Number of Memory&lt;/li&gt;
      &lt;li&gt;Duration of Job, Stage, Tasks&lt;/li&gt;
      &lt;li&gt;Fair scheduler Pool&lt;/li&gt;
      &lt;li&gt;Structured Steaming : max-files, max-size per trigger&lt;/li&gt;
      &lt;li&gt;Shuffle Partitions&lt;/li&gt;
      &lt;li&gt;Shuffle Spill (memory)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Azure Data Explorer&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Ingestion
        &lt;ul&gt;
          &lt;li&gt;MaximumBatchingTimeSpan&lt;/li&gt;
          &lt;li&gt;MaximumNumberOfItems&lt;/li&gt;
          &lt;li&gt;MaximumRawDataSizeMB&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Capacity Policy
        &lt;ul&gt;
          &lt;li&gt;Ingestion capacity&lt;/li&gt;
          &lt;li&gt;Extents merge capacity&lt;/li&gt;
          &lt;li&gt;Extents purge rebuild capacity&lt;/li&gt;
          &lt;li&gt;Export capacity&lt;/li&gt;
          &lt;li&gt;Extents partition capacity&lt;/li&gt;
          &lt;li&gt;Materialized views capacity policy&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Azure Functions (Python)&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;FUNCTIONS_WORKER_PROCESS_COUNT&lt;/li&gt;
      &lt;li&gt;Max instances when Functions scale out&lt;/li&gt;
      &lt;li&gt;Queue Trigger
        &lt;ul&gt;
          &lt;li&gt;batchSize&lt;/li&gt;
          &lt;li&gt;newBatchThreshold&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;HTTP Trigger
        &lt;ul&gt;
          &lt;li&gt;maxConcurrentRequests&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;a href=&quot;https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/azure/data%20pipeline/reflections/2021/01/01/Reflection-of-my-2020-Data-Projects-part2.html&quot;&gt;» Part-II&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://databricks.com/session_na20/best-practices-for-building-robust-data-platform-with-apache-spark-and-delta&quot;&gt;Best Practices for Building Robust Data Platform with Apache Spark and Delta&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/analytics-vidhya/big-data-management-governance-bce5f72821c1&quot;&gt;Analytics Challenges — Big Data Management &amp;amp; Governance&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.sqlshack.com/an-overview-of-etl-and-elt-architecture/&quot;&gt;An overview of ETL and ELT architecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.panoply.io/etl-vs-elt-the-difference-is-in-the-how&quot;&gt;ETL Vs ELT: The Difference Is In The How&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=qHykK5pFRW4&quot;&gt;Intro to Chaos Engineering (Video)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://martinfowler.com/articles/evodb.html#AllDatabaseArtifactsAreVersionControlledWithApplicationCode&quot;&gt;Evolutionary Database Design&lt;/a&gt;
-&lt;a href=&quot;https://phoenixnap.com/blog/chaos-engineering&quot;&gt;Chaos Engineering: How it Works, Principles, Benefits, &amp;amp; Tools
&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Lessons I learned and want to keep in mind in 2021+</summary></entry><entry><title type="html">Lessons learned from building a large scale historical data analysis system using Azure Data Explorer - Part 2</title><link href="https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/kql/azure/2020/05/25/Lession-Learn-LargeScale-ADX-part2.html" rel="alternate" type="text/html" title="Lessons learned from building a large scale historical data analysis system using Azure Data Explorer - Part 2" /><published>2020-05-25T00:00:00-05:00</published><updated>2020-05-25T00:00:00-05:00</updated><id>https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/kql/azure/2020/05/25/Lession-Learn-LargeScale-ADX-part2</id><content type="html" xml:base="https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/kql/azure/2020/05/25/Lession-Learn-LargeScale-ADX-part2.html">&lt;p&gt;[&lt;em&gt;This is the part II of the topic, you can check &lt;a href=&quot;https://herman-wu.github.io/blogs/azure%20data%20explorer%20(kusto)/data/2020/05/21/Lession-Learn-LargeScale-ADX-part1.html&quot;&gt;Part I&lt;/a&gt; if you haven’t.&lt;/em&gt;]&lt;/p&gt;

&lt;p&gt;I have mentioned &lt;a href=&quot;https://herman-wu.github.io/blogs/azure%20data%20explorer%20(kusto)/data/2020/05/21/Lession-Learn-LargeScale-ADX-part1.html&quot;&gt;5 basic lessons&lt;/a&gt; I learned from my recent project that has hundreds of terabytes data. In this &lt;strong&gt;part II&lt;/strong&gt; I would like to share a few more advanced lessons.&lt;/p&gt;

&lt;h5 id=&quot;lesson-6-consider-guiding-users-to-submit-the-right-queries-and-think-about-having-some-data-process-running-on-the-client-side&quot;&gt;Lesson 6 Consider guiding users to submit the right queries and think about having some data process running on the client-side&lt;/h5&gt;

&lt;p&gt;&lt;a href=&quot;https://azure.microsoft.com/en-in/services/data-explorer/&quot;&gt;Azure Data Explorer&lt;/a&gt;(ADX) is designed to process a huge amount of data. It can efficiently handle hundreds of terabytes data or even petabytes data. Data is a very valuable asset especially when you have a lot of them. These data can support users to find hidden business trends, detect anomaly events, identify correlation things from these data.&lt;/p&gt;

&lt;p&gt;However with so much data in your system’s back yard, you also need to make sure users will access them in the right way because users might not have a good sense of how much data they are trying to process. So we should guide and prevent users from unconsciously trying to retrieve 100 gigabytes of data to their web browser or trying to get 1 million records to their client just for showing a line chart. Helping users to use aggregation and return data in just enough granularity for their business needs can largely reduce system workload and improve performance.&lt;/p&gt;

&lt;p&gt;Also you should consider using the user’s client to share some data process loading of ADX. For example, users might want to sort the result by some specific columns, this can be easily implemented on the client-side. It’s also easier to do data paging in client slides then resubmit queries for each page.&lt;/p&gt;

&lt;p&gt;In ADX it provides some mechanisms such as &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/kusto/concepts/querylimits#limit-on-result-set-size-result-truncation&quot;&gt;result set size&lt;/a&gt;, &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/kusto/concepts/querylimits#limit-on-request-execution-time-timeout&quot;&gt;request execution time&lt;/a&gt; to prevent system resource been occupied by some bad queries. You can also use these mechanisms to prevent some queries to take too many resources from the system or grant more resources to some special queries.&lt;/p&gt;

&lt;h5 id=&quot;lesson-7-check-some-advance-table-policy&quot;&gt;Lesson 7 Check some advance table policy&lt;/h5&gt;

&lt;p&gt;To further provide better query performance, ADX now provides some new more advanced configuration in Table policies such as &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/kusto/management/partitioningpolicy&quot;&gt;Data partitioning policy&lt;/a&gt; and &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/kusto/management/roworder-policy&quot;&gt;RowOrder policy&lt;/a&gt;. If you need to improve query response time, they are options you can try. But using them carefully, &lt;strong&gt;they will increase data ingestion time and consume more computing resources&lt;/strong&gt;. Also, partition has some side effect on extents management and need more resources to run the policy.&lt;/p&gt;

&lt;p&gt;The other consideration is the choice between putting data in different tables or different databases. This choice will impact the following areas in system design:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Data Access Control&lt;/li&gt;
  &lt;li&gt;Admin node workload&lt;/li&gt;
  &lt;li&gt;Data sharing mechanism&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Users should consider putting data in different databases when you want to have more flexibility to manage and share data. Also when the data volume is big, having multiple databases can enjoy the benefit of having different Admin nodes for each database and then have more resources to maintaining data metadata and perform transactions.&lt;/p&gt;

&lt;h5 id=&quot;lesson-8-check-server-loading-and-very-very-carefully-tuning-some-of-them-if-needed&quot;&gt;Lesson 8 Check Server loading and very very carefully tuning some of them if needed.&lt;/h5&gt;

&lt;p&gt;To handle data ingestion and query workload, ADX has several mechanisms to balance the resources used to support different operation and maintenance workload.&lt;/p&gt;

&lt;p&gt;For data management operations, you can use &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/kusto/management/capacitypolicy&quot;&gt;.show capacity&lt;/a&gt; to quickly show the resources utilization of these core capabilities.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2020-05-25-Lession-Learn-LargeScale-ADX-part2/show_capacity.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In most cases the official recommendation is &lt;strong&gt;just leave the setting as is, the default setting should be able to handle most of the situations&lt;/strong&gt; and they will keep each resource utilization balanced. If in some cases if you thought you really need to change these values, it’s also recommended to at least check and ask in &lt;a href=&quot;https://stackoverflow.com/questions/tagged/kusto&quot;&gt;stackoverflow&lt;/a&gt; to have some experts to double verify it.&lt;/p&gt;

&lt;p&gt;Also you might also want to check the Extents number and its growth in your system. Extents are the core data allocation unit within ADX, ADX will try to organize it in the best way to fulfill both data ingestion needs and data query needs. If for some reason too much extends are created, it will increases the loading of ADX admin node and potential will impact system performance. For a large system it’s a good practice to keep an eye on it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2020-05-25-Lession-Learn-LargeScale-ADX-part2/total_extents.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;lesson-9-optimize-the-kql-query-plan-for-the-long-queries-and-review-cache-policy&quot;&gt;Lesson 9 Optimize the KQL query plan for the long queries and review cache policy&lt;/h5&gt;
&lt;p&gt;Though we mentioned how different query syntax could impact query performance in  &lt;a href=&quot;https://herman-wu.github.io/blogs/azure%20data%20explorer%20(kusto)/data/2020/05/21/Lession-Learn-LargeScale-ADX-part1.html&quot;&gt;Understand and test KQL query performance session of Part I&lt;/a&gt;, here I want to talk about KQL query plan.&lt;/p&gt;

&lt;p&gt;In Kusto Explorer, there is a build-in tool named Query Analyzer which provides extra information on how your KQL runs. You can use them to understand how the query actually being executed, if your query can run in parallel on all the nodes, if the query filter efficiently reduces data retrieval volumes or if the data join.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2020-05-25-Lession-Learn-LargeScale-ADX-part2/execprofile.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2020-05-25-Lession-Learn-LargeScale-ADX-part2/reloptree.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Also by design initially (before the cluster is running) all the data are stored on cold storage (Azure Blob storage). When the cluster starts, cluster nodes will load all these hot data (defined by &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/kusto/management/cachepolicy&quot;&gt;Cache policy&lt;/a&gt;) data into local SSD and memory so they become hot data and can be quickly accessed. Review the cache hit rate of popular queries and check if the cache policy is properly defined can potentially have a good boost of performance.&lt;/p&gt;

&lt;h5 id=&quot;lesson-10-review-security-setting&quot;&gt;Lesson 10 Review Security Setting&lt;/h5&gt;

&lt;p&gt;Last but not least, Security. ADX can use both Azure AD and Microsoft Account (MSAs) as the identity providers. And we also need to have a plan for user authentication and &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/kusto/management/access-control/how-to-provision-aad-app&quot;&gt;application authentication&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are different &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/kusto/management/access-control/role-based-authorization&quot;&gt;security roles&lt;/a&gt; in ADX. It’s good to review the security setting in your system and make sure you have separation of concerns for your account and security planning.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Azure Data Explorer is a very powerful tool for analyzing historical log or telemetries data. In many cases we saw it provide a more cost-effective and more powerful query tool to fulfill users’ business needs.&lt;/p&gt;

&lt;p&gt;Above are 10 lessons I thought are important when implementing a large scale data analysis platform when using Azure Data Explorer. The best way to understand the detail of them is still read through the official documents. But these lessons might provide you a quick bird’s-eye view of what you should check and look.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Reference]&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/Azure-Samples/kusto-high-scale-ingestion/blob/master/processing/README.md&quot;&gt;- kusto-high-scale-ingestion&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://azure.microsoft.com/en-ca/resources/azure-data-explorer/&quot;&gt;- Azure Data Explorer technical white paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://yonileibowitz.github.io/kusto.blog/blog-posts/update-policies.html&quot;&gt;- Update policies for in-place ETL in Kusto (Azure Data Explorer)&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">[This is the part II of the topic, you can check Part I if you haven’t.]</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://azurecomcdn.azureedge.net/cvt-ee71595d3667788def73479da1629d673313a0b081e460fc596839b82f34a2df/images/page/services/machine-learning/mlops/steps/mlops-slide1-step3.svg" /><media:content medium="image" url="https://azurecomcdn.azureedge.net/cvt-ee71595d3667788def73479da1629d673313a0b081e460fc596839b82f34a2df/images/page/services/machine-learning/mlops/steps/mlops-slide1-step3.svg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Lessons learned from building a large scale historical data analysis system using Azure Data Explorer - Part 1</title><link href="https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/kql/azure/2020/05/21/Lession-Learn-LargeScale-ADX-part1.html" rel="alternate" type="text/html" title="Lessons learned from building a large scale historical data analysis system using Azure Data Explorer - Part 1" /><published>2020-05-21T00:00:00-05:00</published><updated>2020-05-21T00:00:00-05:00</updated><id>https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/kql/azure/2020/05/21/Lession-Learn-LargeScale-ADX-part1</id><content type="html" xml:base="https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/kql/azure/2020/05/21/Lession-Learn-LargeScale-ADX-part1.html">&lt;p&gt;Recently I had an opportunity to participate in another data project that also uses &lt;a href=&quot;https://azure.microsoft.com/en-in/services/data-explorer/&quot;&gt;Azure Data Explorer&lt;/a&gt;(ADX) as the core data process and store engine. In this project we used ADX to ingest and process more than half of petabytes data. Like most projects we were under some time and resource constraints, and also encountered a few unexpected technical challenges due to the constraints. Though we couldn’t implement the system using the best-optimized architecture (it will take too much time than the project was allowed), we still managed to achieve the project goal. It’s an exciting and fun journey and here are a few lessons we learned.&lt;/p&gt;

&lt;h5 id=&quot;lesson-1-select-proper-sku&quot;&gt;Lesson 1 Select proper SKU&lt;/h5&gt;

&lt;p&gt;There are couples different &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/manage-cluster-choose-sku&quot;&gt;SKU options for ADX&lt;/a&gt;, some are more CPU optimized like D-Series (D-series, Ds-series) VM, they have more powerful CPU; some are more storage optimized like Ls-Serious VM, they are equipped with larger SSD to achieve more I/O performance. &lt;em&gt;Have a testing plan to test the key user query patterns on these different type of VMs and check which one is best for your query workload can benefit the project in the long run.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2020-05-21-Lession-Learn-LargeScale-ADX-part1/ADX_SKU.JPG&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;lesson-2-check-different-ingestion-options&quot;&gt;Lesson 2 Check different ingestion options.&lt;/h5&gt;

&lt;p&gt;In Azure Data Explorer, it supports several different ingestion solutions, the decision will depend on the purpose and stage of your development. You can check &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/kusto/management/data-ingestion/#ingestion-methods&quot;&gt;here&lt;/a&gt; for detail information of these solutions.&lt;/p&gt;

&lt;p&gt;It’s better to read through the official document, understand their differences before making a decision. Meanwhile, here are a few thumb rules:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For query testings, verifying scripts, tables, you can use &lt;strong&gt;Inline ingestion (push)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;For ad-hoc feature engineering, data cleaning, you can use &lt;strong&gt;Ingest from query&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;For ingestion testing, create some volumes of data, you can use &lt;strong&gt;Ingest from storage (pull)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;For production ingestion pipeline testing, I normally will use &lt;strong&gt;Queued ingestion&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition to the above basic ingestion options, you can also check the following options based on your scenario and environment.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/kusto/management/data-ingestion/eventgrid&quot;&gt;Ingest from storage using Event Grid subscription&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/kusto/management/data-ingestion/eventhub&quot;&gt;Ingest from Event Hub&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/kusto/management/data-ingestion/iothub&quot;&gt;Ingest from IoT Hub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And there is a new ingestion option &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/ingest-data-one-click&quot;&gt;One-Click Ingestion&lt;/a&gt; which are just been announced in &lt;a href=&quot;https://mybuild.microsoft.com/&quot;&gt;Microsoft Build 2020&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Eeek, a lot of choices. :p&lt;/p&gt;

&lt;h5 id=&quot;lesson-3-have-a-clean-ingestion-pipeline&quot;&gt;Lesson 3 Have a clean ingestion pipeline&lt;/h5&gt;

&lt;p&gt;Normally when trying to ingest data into a data repository, we might need to do some data pre-process such as check file format, clean dirty data, do a few data transformation, etc. Azure Data Explorer provides some of these capabilities through &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/kusto/query/functions/user-defined-functions&quot;&gt;User-defined function&lt;/a&gt; and &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/kusto/management/update-policy&quot;&gt;update policy&lt;/a&gt;. You can use these mechanisms to quickly perform some data pre-process tasks within ADX without setup extra computing services to handle it.&lt;/p&gt;

&lt;p&gt;While these are convenient ways to massage data, it will occupy ADX’s resources and potentially introduce more data fragmentation. There are complex mechanisms within ADX to optimize the resources it has, maintains and organize data in its storage and keep the system in a healthy status.&lt;/p&gt;

&lt;p&gt;Under the condition that data ingestion volume is big, these data pipeline activities could impact the resource available for ADX to handle queries or do internal housekeeping tasks. You might need to carefully monitor ADX status and do a few fine tunes on its configuration. While I did use these mechanisms for other smaller-scale projects and love it, &lt;em&gt;it is still a better practice just to keep the data pre-process tasks outside of ADX in large scale project, at least before you are very familiar each internal mechanisms within ADX.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Data Factory, Databricks, Azure Functions/App services, AKS, HDInsight provide good foundational capabilities to pre-process data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2020-05-21-Lession-Learn-LargeScale-ADX-part1/Ingest.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://github.com/Herman-Wu/ADXAutoFileIngestion&quot;&gt;my git project&lt;/a&gt; I shared some codes that I used Azure Functions to do Queued ingestion.&lt;/p&gt;

&lt;h5 id=&quot;lesson-4-evaluate-the-horizontal-scale--of-servers-needed&quot;&gt;Lesson 4 Evaluate the horizontal scale (# of servers) needed&lt;/h5&gt;

&lt;p&gt;When planning system roll-out, a solid estimation of the number of servers needed and a well-estimated expansion plan for the future is important. It can also help save costs by preventing under-utilization and provide valuable information for system design. System &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/manage-cluster-horizontal-scaling&quot;&gt;scale out/ horizontal scaling&lt;/a&gt; is one of the core capabilities that we can make sure the system can be adaptive to the workload and provide just enough resource to the users. In our test, one of the key ADX features that users love is it can provide almost linear performance growth when scaling out. ADX also provides non-destructive services when scaling out.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/media/manage-cluster-horizontal-scaling/manual-scale-method.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;It’s suggested that you should run a few workload simulations and test about how the scale-out can increase your system capabilities.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2020-05-21-Lession-Learn-LargeScale-ADX-part1/QueryPerf01.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ACI and AKS can be helpful if you want to simulate the system workload.&lt;/p&gt;

&lt;h5 id=&quot;lesson-5-understand-and-test-kql-query-performance&quot;&gt;Lesson 5 Understand and test KQL query performance&lt;/h5&gt;

&lt;p&gt;One of the key strengths of ADX is its powerful query language &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/kusto/query/&quot;&gt;Kusto Query Language&lt;/a&gt; (KQL). Like most other data query languages, some query operators in KQL are similar, give you the same result but could have a huge difference in performance.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;It’s always good the validate what’s your key query scenarios and try how different query syntax impact the query performance.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2020-05-21-Lession-Learn-LargeScale-ADX-part1/KustoExpQuery.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can review the query performance using &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/kusto/tools/kusto-explorer&quot;&gt;Kusto.Explorer tool&lt;/a&gt; or &lt;a href=&quot;https://dataexplorer.azure.com/&quot;&gt;ADX Web UI&lt;/a&gt;. You can also use &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/kusto/management/queries&quot;&gt;.show queries&lt;/a&gt; operator to review the performance of historical queries.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2020-05-21-Lession-Learn-LargeScale-ADX-part1/ShowQueries.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;.. to be continued&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/kql/azure/2020/05/25/Lession-Learn-LargeScale-ADX-part2.html&quot;&gt;Lessions learned from buiding a large scale historical data analysis system using Azure Data Explorer - Part 2&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">Recently I had an opportunity to participate in another data project that also uses Azure Data Explorer(ADX) as the core data process and store engine. In this project we used ADX to ingest and process more than half of petabytes data. Like most projects we were under some time and resource constraints, and also encountered a few unexpected technical challenges due to the constraints. Though we couldn’t implement the system using the best-optimized architecture (it will take too much time than the project was allowed), we still managed to achieve the project goal. It’s an exciting and fun journey and here are a few lessons we learned.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://azurecomcdn.azureedge.net/cvt-ee71595d3667788def73479da1629d673313a0b081e460fc596839b82f34a2df/images/page/services/machine-learning/mlops/steps/mlops-slide1-step3.svg" /><media:content medium="image" url="https://azurecomcdn.azureedge.net/cvt-ee71595d3667788def73479da1629d673313a0b081e460fc596839b82f34a2df/images/page/services/machine-learning/mlops/steps/mlops-slide1-step3.svg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Troubleshooting Azure Data Explorer (Kusto) cross-tenant access issues</title><link href="https://herman-wu.github.io/blogs/azure%20data%20explorer%20(kusto)/data/2020/03/17/TroubleShootingADXLoginIssues.html" rel="alternate" type="text/html" title="Troubleshooting  Azure Data Explorer (Kusto) cross-tenant access issues" /><published>2020-03-17T00:00:00-05:00</published><updated>2020-03-17T00:00:00-05:00</updated><id>https://herman-wu.github.io/blogs/azure%20data%20explorer%20(kusto)/data/2020/03/17/TroubleShootingADXLoginIssues</id><content type="html" xml:base="https://herman-wu.github.io/blogs/azure%20data%20explorer%20(kusto)/data/2020/03/17/TroubleShootingADXLoginIssues.html">&lt;h5 id=&quot;azure-data-explorer-introduction&quot;&gt;Azure Data Explorer Introduction&lt;/h5&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/data-explorer-overview&quot;&gt;Azure Data Explorer (ADX, aka Kusto)&lt;/a&gt; is a very powerfully log/historical data analysis platform provided by Microsoft that powers several key Azure services such as Application Insight, Azure Monitor, Time Series insight. It is designed to handle huge amounts of historical data and can ingest and process Peta-bytes of data every day with little efforts to set up the infrastructure. On February 7, 2019, Microsoft GA the service to customers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2020-03-17-TroubleShootingADXLoginIssues/ADXOverview.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One of the features that I particularly like is its query language KQL (Kusto Query Language), KQL combines the concept of SQL query and data pipeline. In real project experience, it is very powerful and saved me a lot of time to get the query result in the structure that my projects need. You can check &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/kusto/query/tutorial?pivots=azuredataexplorer&quot;&gt;this official tutorial&lt;/a&gt; that introduces KQL’s key concept and following is an example about how to query data 12 days ago and do count aggregation very 30 minutes. It’s a very popular bin count pattern when analyzing data on time dimension. In the query we use &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/kusto/query/mvexpandoperator&quot;&gt;“mv-expand”&lt;/a&gt; operator to make sure there is still a record presents the 0 count even the system has no data in that 30 minutes range. “mv-expand” is also very useful when you are parsing and expanding JSON data.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;let StartTime=ago(12d);
let StopTime=ago(11d);
T
| where Timestamp &amp;gt; StartTime and Timestamp &amp;lt;= StopTime 
| summarize Count=count() by bin(Timestamp, 30m)
| union ( // 1
  range x from 1 to 1 step 1 // 2
  | mv-expand Timestamp=range(StartTime, StopTime, 30m) to typeof(datetime) // 3
  | extend Count=0 // 4
  )
| summarize Count=sum(Count) by bin(Timestamp, 30m) // 5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2020-03-17-TroubleShootingADXLoginIssues/kqlqueryresult.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can also easily generate a graphic chart to visualize your query result through &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/kusto/query/renderoperator?pivots=azuredataexplorer&quot;&gt;“render”&lt;/a&gt; operator. In the previous query, you can add “| render timechar” to generate the following graph which represents the trend of how many records we have every 30 mins eleven days ago.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;|render timechart 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2020-03-17-TroubleShootingADXLoginIssues/kqlquerygraph.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For data consumers ADX also provides various client tool that users can use to create different kind of query, graphic chart and dashboard. The tool that support ADX includes :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/web-query-data&quot;&gt;ADX WebUI&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/kusto/tools/kusto-explorer&quot;&gt;Kusto Desktop Client tool&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;PowerBI&lt;/li&gt;
  &lt;li&gt;Excel&lt;/li&gt;
  &lt;li&gt;Jupyter Notebook(&lt;a href=&quot;https://github.com/microsoft/jupyter-Kqlmagic&quot;&gt;using KQLMagic extension&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/grafana&quot;&gt;Grafana&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Azure/azure-kusto-spark&quot;&gt;Spark&lt;/a&gt;   .&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;azure-data-explorer-access-control&quot;&gt;Azure Data Explorer Access Control&lt;/h5&gt;

&lt;p&gt;In the access control part, ADX supports user authentication through Microsoft Accounts (MSAs) and Azure AD account. Microsoft Account is non-organizational user accounts, these accounts normally use email like hotmail.com, live.com, outlook.com. Azure AD account is created through Azure AD or another Microsoft cloud service such as Office 365. It will be tight to an Azure AD tenant and is the preferred method for authenticating to ADX.  Most enterprise users should use AAD account for authentication. Here has more information about &lt;a href=&quot;https://docs.microsoft.com/zh-tw/azure/kusto/management/access-control/&quot;&gt;ADX access control&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Like most Azure services, you can manage user accounts and their access to ADX through “Access control” function within Azure Portal.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2020-03-17-TroubleShootingADXLoginIssues/ADXAccessControl.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The other way to manage it is through KQL query. You can run the following command to check the accounts and roles that can access ADX database.&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;show&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;database&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;Database&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;principals&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can read ADX &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/kusto/management/security-roles&quot;&gt;Security roles management&lt;/a&gt; session to understand more about using  KQL to manage user access control.&lt;/p&gt;

&lt;p&gt;ADX authentication is also the part I would like to share a few troubleshooting tips which  I learned from projects.&lt;/p&gt;

&lt;h5 id=&quot;azuer-data-explorer-cross-tenant-access-issue&quot;&gt;Azuer Data Explorer cross-tenant access issue&lt;/h5&gt;

&lt;p&gt;Ideally if you login your PC with Azure AD account that in the same tenant of the Azure subscription which been used to create the ADX cluster, then everything should work good and you can just management user access in Azure Portal. However it’s not always the case, users can come from different organizations and partners. Here are a few ways we used to check and fix the issues.&lt;/p&gt;

&lt;h5 id=&quot;the-problem&quot;&gt;The problem:&lt;/h5&gt;
&lt;p&gt;&lt;em&gt;Grant a user to access ADX in Azure Portal UI. And the user can assess ADX in Azure portal. But he/she but couldn’t access it in &lt;a href=&quot;(https://docs.microsoft.com/en-us/azure/data-explorer/web-query-data)&quot;&gt;ADX Web UI&lt;/a&gt; and &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/kusto/tools/kusto-explorer&quot;&gt;Kusto Explorer&lt;/a&gt;, PowerBI, Excel or other client tools.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;What happened is when you grant a new user to access ADX, if the user’s account comes from a different tenant, in ADX it will create a new Principle Object Id in its tenant. If the user access ADX in Azure portal, the user’s account already switches to the right tenant so there is no problem accessing ADX. But if users from different tenant try to use other client tools, these client tools will use default tenant and which might not match the tenant that ADX is using.&lt;/p&gt;

&lt;h5 id=&quot;how-to-fix-the-issue&quot;&gt;How to fix the issue&lt;/h5&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Solution A&lt;/strong&gt;: Force ADX WebUI and Kusto Explorer to use not ADX’s tenant&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ADX WebUI&lt;/p&gt;

    &lt;p&gt;Default ADX WebUI has URL like&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;https://dataexplorer.azure.com/clusters/[cluster name].[data center]&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;You can force it to recognize a user’s login tenant by add &amp;amp;tenant=[tenant id] to the URL. So it will look like&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;https://dataexplorer.azure.com/clusters/[cluster name].[data center]?tenant=[tenant id]&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Kusto Explorer Client&lt;/p&gt;

    &lt;p&gt;Instead of default connection by specifying ADK cluster URL, You need to use advanced query.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2020-03-17-TroubleShootingADXLoginIssues/KustoExpConn.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The connection string will be like:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  Data Source=https://[Cluster Name].[Data Center].kusto.windows.net;AAD Federated Security=True;AAD User ID=[User's AAD account(E-mail)];Authority Id=[User's AAD account tenant ID ]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Solution B&lt;/strong&gt;: Grant user using ‘right’ tenant id and account id&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As said by default user granted in Azure portal will using ADX’s tenant id and create a new account object id. If you want to add a user from a different tenant, the suggested way will be to grant users using KQL.&lt;/p&gt;

&lt;p&gt;You can grant a new user with different roles like&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-kql&quot;&gt;.add database [Database Name] [Role Name]  ('aaduser=[User AAD account id];[User AAD tenant id]') 'Notes for the account, eg. User Name'

&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Get AAD account object id &amp;amp; tenant id using Azure CLI&lt;/p&gt;

    &lt;p&gt;To get user’s AAD account object id, you can use Azure CLI command :&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  az ad signed-in-user show 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To get the user’s tenant ID, you can use Azure CLI command :&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  az account list 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Get AAD account object id &amp;amp; tenant id in Azure portal&lt;/p&gt;

    &lt;p&gt;You can also find AAD account object id &amp;amp; tenant id in Azure Active Directory service of  Azure portal.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2020-03-17-TroubleShootingADXLoginIssues/AAD.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Get AAD account object id &amp;amp; tenant id in ADX Web UI&lt;/p&gt;

    &lt;p&gt;If you login using ADX Web UI, actually the error message contains the AAD user account id and tenant id. The message will be looks like following&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Error message : 
action Principal ‘aaduser=[&lt;strong&gt;AAD accound id&lt;/strong&gt;];[ &lt;strong&gt;AAD tenant id&lt;/strong&gt; ]’ is not authorized to perform operation&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;It’s another easy way to get the information you needed.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
ADX is a very powerful platform that provides interactive analysis capabilities which can handle huge amount of historical log data with little infrastructure maintenance efforts. Because most enterprises use Azure AD to grant users access, helping these tips can save you sometime when you have cross tenant access requirements.&lt;/p&gt;</content><author><name></name></author><summary type="html">Azure Data Explorer Introduction</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://azurecomcdn.azureedge.net/cvt-ee71595d3667788def73479da1629d673313a0b081e460fc596839b82f34a2df/images/page/services/machine-learning/mlops/steps/mlops-slide1-step3.svg" /><media:content medium="image" url="https://azurecomcdn.azureedge.net/cvt-ee71595d3667788def73479da1629d673313a0b081e460fc596839b82f34a2df/images/page/services/machine-learning/mlops/steps/mlops-slide1-step3.svg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Test</title><link href="https://herman-wu.github.io/blogs/2019/03/03/Test.html" rel="alternate" type="text/html" title="Test" /><published>2019-03-03T00:00:00-06:00</published><updated>2019-03-03T00:00:00-06:00</updated><id>https://herman-wu.github.io/blogs/2019/03/03/Test</id><content type="html" xml:base="https://herman-wu.github.io/blogs/2019/03/03/Test.html">&lt;p&gt;Test Images&lt;/p&gt;

&lt;p&gt;This is a Test&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Herman-Wu.github.io/blogs/assets/img/2019-03-03-Test/media/image1.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/Herman-Wu/blogs/blob/master/images/diagram.png?raw=true&quot;&gt;https://github.com/Herman-Wu/blogs/blob/master/images/diagram.png?raw=true&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/img/2019-03-03-Test/media/image2.png&quot; alt=&quot;Screenshot&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">Test Images</summary></entry></feed>