{
  
    
        "post0": {
            "title": "Security practices and design priciples for implementing a data lakehouse solution in Azure Synapse",
            "content": "Security practices and design priciples for implementing a data lakehouse solution in Azure Synapse . Background . &quot;The Lake House&quot; by YellowstoneNPS. Licensed under CC PDM 1.0 . Synapse is a versatile data platform that supports enterprise data warehousing, realtime data analytics, data pipeline, time-serious data processing, machine learning and data governance. It integrates several different technologies (e.g., SQL DW, Serverless SQL, Spark, Data Pipeline, Data Explorer, Synapse ML, Purview…) to support these various capabilities. However, this also inevitably increases the complexity of the system infrastructure. . . In this blog I would like to share the security learning after implemented a data lakehouse project for a international manufacture company using Synapse. Data Lakehouse is a modern data management architecture that combines data lakes’s cost-efficiency, scale, and flexibility features with data warehouses’s data and transaction management capabilities. It well supports business intelligence and machine learning (ML) scenario for large amount of diverse data structure and data source.Some common use cases for the solution are IoT telemetry analysis, commsumer activities and behaveior tracking, security log monitoring, or semi-structured data processing. . We will focus on the security design and implementation pratices used in the project. In the project we chose Synapse serverless SQL and Synpase Spark to implemente the data lakehouse pattern. Following is the high level solution design architecture. . . Fig.1 High level concept of the solution Source: The best practices for organizing Synapse workspaces and lakehouse Design Focus . We started the security design by using the Threat Modeling tool. The tool helps us communicate with project stakeholders about the protecial risks, and define the trust boundary in the system. Based on the thread modeling result, Identity and Access control, Network protection, and DevOps security are proirotized in the project. Based on these priorities, we implemented additional security features and changed the infrastures so we can protect the system and mitigate key secruity risks identified. These also map to Access Control, Asset Protection, and Innovation Security in the Cloud Adoption Framework (CAF)’s security disciplines. We will walk through the design priciples and related technologies in more detail In the following sections. . Security Design Principles and Learning . Network and Asset protection Design . One of the key security assurances principles in the Cloud Adoption Framework is the Zero Trust principle. When designing security for any component or system, we should reduce the risk of attackers expanding access by assuming other resources in the organization are compromised. . Based on the threat modeling discussion result, we follow the micro-segmentation deployment recommendation in zero-trust and defined several security boundaries. VNet and Synapse data exfiltration protection are the key technologies used to implement the security boundary and protect the system’s data assets and critical components. . Considering Synapse is a composition of several different technologies, we need to : . Identify essential components of Synapse and related services used in the project. . Synapse is a very versatile data platform. It can handle and fulfill many different data processing needs. First, we need to decide which components in Synapse are used in the project to plan how to protect them. Also, we need to determine what other services are communicating with these Synapse’s components. In the data data lakehouse architecture, Synapse Serverless SQL, Synapse Spark, Synpase Pipeline,Azure Data Lakes and Azure DevOps are the key components. . | Define the __legal communication behaviors__ between the components. . We need to define the “legal” communication behaviors between the components. For example, do we want the Synapse Spark engine to communicate with the dedicated SQL instance directly, or do we want the spark engine to communicate with the database through a proxy such as Synapse Data Integration pipeline or Data Lake? . Base on the Zero trust principle, we should block the communication if there is no business need for the interaction. For example, we should block the communication if a Synapse Spark engine directly communicates with Data Lake storage in an unknown tenant. . | Chose the proper security solution that can enforce the defined communication behaviors. . In Azure, several security technologies are capable of enforcing the defined service communication behaviors. For example, in Azure Data Lake storage, you can use a white-list IP address to control its access, but you can also choose allowed VNet, Azure services, or resource instances. Each protection method provides a different security protection and needs to be selected based on the business needs and environment limitation. I will decribe the configuration we used in our project in the next section. . | Add threat detection and advanced defense for critical resources. . For critical resources, it is better to add threat detection and advanced defense. These services help identify threats and triggers alerts. So the system can notify uses about the security breach. . | . Network and Asset protection Implementation in the project . In the data lakewarehouse solution, we designed and controlled the service’s interaction behaviors based on business requirements to mitigate security threats. The following table shows the defined communication behaviors and security solutions used in the project. . From (Client) To (Service) Behavior Configuration Notes   . Internet | Azure DataLake | Deny All | Firewall Rule - Default Deny | Default: ‘Deny’ | Firewall Rule - Default Deny | . Synapse Pipeline/Spark | Azure DataLake | Allow (Instance) | VNet - Managed Private EndPoint (Azure DataLake) |   |   | . Synapse SQL | Azure DataLake | Allow (Instance) | Firewall Rule - Resource instances (Synapse SQL) | Synapse SQL needs to access Azure DataLake using Managed Identity | N/A | . Azure Pipeline Agent | Azure DataLake | Allow (Instance) | * Firewall Rule - Selected Virtual networks * Service Endpoint - Storage | For Integration Testing bypass: ‘AzureServices’ (firewall rule) |   | . Internet | Synapse Workspace | Deny All | Firewall Rule |   | Firewall Rule | . Azure Pipeline Agent | Synapse Workspace | Allow (Instance) | VNet - Private EndPoint | Requires 3 Private EndPoints (Dev, Serverless SQL, and Dedicate SQL) |   | . Synapse Managed VNet | Internet/ Unauthorized Azure Tenant | Deny All | VNet - Synapse Data Exfiltration Protection |   |   | . Synaspe Pipeline/Spark | KeyVault | Allow (Instance) | VNet - Managed Private EndPoint (KeyVault) | Default: ‘Deny’ |   | . Azure Pipeline Agent | KeyVault | Allow (Instance) | * Firewall Rule - Selected Virtual networks * Service Endpoint - KeyVault | bypass: ‘AzureServices’ (firewall rule) |   | . Azure Functions | Synapse Serverless SQL | Allow (Instance) | VNet - Private EndPoint (Synapse Serverless SQL) |   |   | . Synaspe Pipeline/Spark | Azure Monitor | Allow (Instance) | VNet - Private EndPoint (Azure Monitor) |   |   | . The bellow diagram shows the architecture with the network and asset protection design. . For example, the above diagram includes: . Create a Synapse workspace with a managed virtual network. | Securing data egress from Synapse workspaces through Synapse workspaces Data exfiltration protection. | Manage the list of approved Azure AD tenants for the Synapse workspace. | Configure network rules to grant only traffic from selected virtual networks access to storage account and disable public network access. | Use Managed Private Endpoints to connect Synapse managed VNet with Data Lake. | Use Resource Instance to securely connect Synapse SQL with Data Lake | . For better Network and Asset protection, the following are additional security design considerations. . Deploy Perimeter Networks for Security Zones for Data Pipeline. . Because in a data pipeline, data could be loaded from external data sources. When a data pipeline workload requires access to external data and data landing zone, it is better to implement a perimeter network and separate it with a regular ETL pipeline. . | Enable Azure Defender for all storage accounts. . Azure Defender provides an additional layer of security intelligence that detects unusual and potentially harmful attempts to access or exploit storage accounts. Security alerts are triggered in Azure Security Center. . | Lock storage account to prevent malicious deletion or configuration changes . | . Identity and Access control . There are several parts in the system, each part requires different Identity and Access Management (IAM) configuration. They will need to collaborate tightly with each other to provide a streamlined user experience. Therefore, when we implement authentication and authorization control, we need to plan the following parts. . . Chose Identity type in different Access Control Layers . There are four different identity solutions in the system. . SQL Account (SQL Server) | Service Principal (Azure AD) | Managed Identity (Azure AD) | User AAD Account (Azure AD) | . Also, there are four different access control layers in the system. . Application access layer | Synapse access layer | SQL DB access layer | Azure Data Lake access layer | . A key part of identity and access control is choosing the right identity solution for different user roles in each access control layer. The Well Architecture Security Design Principles suggests using native controls and driving simplicity. Therefore, we decided to use User AAD account in the Application, Synapse, and SQL DB access layer to leverage the native first-party IAM solutions while using Managed Identity of Synapse to access Azure Data Lake to simply the authorization process. . | Consider Least-privileged access . In Zero trust guiding principles, it suggests providing just-in-time and just-enough-access to critical resources. Azure AD Privileged Identity Management (PIM) and enhance security deployment check in the future. . | Protect Linked Service . Linked services define the connection information needed for the service to connect to external resources. It is important to secure the linked Service configuration and access in Synapse. . Create Azure Data Lake linked service with Private Links | Use Managed Identity as the authentication method in linked services | Use Azure Key Vault as the secret stored in Linked Service. | . | . DevOps security . Use VNet enabled self-hosted pipeline agent . Default Azure DevOp pipeline agent couldn’t support VNet communication because it used a very wide IP range. Therefore, we implemented Azure DevOps self-hosted agent in VNet so the DevOps process can be protected and smoothly communicate with the whole system. In addition, VM scale sets are used to ensure the DevOps engine can scale up and down. . | . . Implement infrastructure security scanning &amp; security smoke testing in CI/CD pipeline . Static analysis tool for scanning infrastructure as code (IaC) files can help detect and prevent misconfigurations that may lead to security or compliance problems. In addition, security smoke testing ensures that the vital system security measure is successfully enabled and prevents security risk due to a design fault in the deployment pipeline. . Use static analysis tool for scanning infrastructure as code (IaC) templates to detect and prevent misconfigurations that may lead to security or compliance problems. Use tools such as Checkov or Terrascan to detect and prevent security risks. | Make sure the CD pipeline correctly handles the failure of the deployment. Any deployment failure related to security features should be treated as a critical failure. It should retry the failed action or hold the deployment. | Validate the security measures in the deployment pipeline by running security smoke testing. The security smoke testing, such as validating the configuration status of deployed resources or testing cases that examine critical security scenarios, can ensure that the security design is working as expected. | . | . Security Score assessment and Treat Detection . To understand the security status of the system, we used Microsoft Defender for Cloud to assess the infrastructure security and detect the security issues. Microsoft Defender for Cloud is a tool for security posture management and threat protection. It can protect workloads running in Azure, hybrid, and other cloud platforms. . . You can enable Defender for Cloud’s free plan on all your current Azure subscriptions when you visit the Defender for Cloud pages in the Azure portal for the first time. I highly recommend to enable it so you can get your Cloud security posture evaluation and suggestions. And it’a all free, so why not ☺️. Microsoft Defender for Cloud will provide you security score and some security hardening guidance for your subscription. . . The information is quite strait forward and the recommendations are very good. Many of the recommendations can are easy to take action. . . And if you need advanced security management and threat detection capabilities, which provide fetures such as suspicious activities detection and alerting. You can enable Cloud workload protection indivually for different resources. So you have the option to choose the most cost effective way to protect the system. . Summary . With the combination of Synapse Serverless SQL and Synpase Spark, we built a flexible, scalable and cost-effective data lakehouse solution in the project. In this article, I tired to summarized the security design principle and practices we implemented in the solution. We start from the Threat Modeling tool and guidance in the Cloud Adoption Framework (CAF)’s security disciplines. To harden the security protection, the project team decided to focus on Network and Asset protection, Identity and Access control, and DevOps security. We also evaluated the security score of the system and review the security suggestions provided by Microsoft Defender for cloud. There are several different configurations you need to choose when you want to protect the network. In article it also describes the design after we compared different options. Identity and Access control is very important for securing the data asset in the system. Especially for the data lakehouse solution, you need to mapping different access control layers and choose right identity solution for it. Hope this learning will also helps you to implementing a secure data lakehouse solution on Azure Synapse. . . Reference . Current Data Patterns Blog Series: Data Lakehouse | The Data Lakehouse, the Data Warehouse and a Modern Data platform architecture | The best practices for organizing Synapse workspaces and lakehouse | Secure score in Microsoft Defender for Cloud | Understanding Azure Synapse Private Endpoints | Azure Synapse Analytics – New Insights Into Data Security | Azure security baseline for Azure Synapse dedicated SQL pool (formerly SQL DW) | Cloud Network Security 101: Azure Service Endpoints vs. Private Endpoints | How to set up access control for your Synapse workspace | Connect to Azure Synapse Studio using Azure Private Link Hubs | How-To Deploy your Synapse Workspace Artifacts to a Managed VNET Synapse Workspace | Continuous integration and delivery for an Azure Synapse Analytics workspace | .",
            "url": "https://herman-wu.github.io/blogs/datalakehouse/lakehouse/synapse/security/data/sql/serverless%20sql/azure/vnet/private%20endpoint/2022/03/15/Security-practices-and-design-priciples-for-implementing-a-data-lakehouse-solution.html",
            "relUrl": "/datalakehouse/lakehouse/synapse/security/data/sql/serverless%20sql/azure/vnet/private%20endpoint/2022/03/15/Security-practices-and-design-priciples-for-implementing-a-data-lakehouse-solution.html",
            "date": " • Mar 15, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Reflections on my 2020 Data Projects  - Part II",
            "content": "Lessons I learned and want to keep in mind in 2021+ . « Part-I . #6 Every small problem can be BIG . Image by Evgeni Tcherkasski from Pixabay . When we were developing solutions for different projects, the dev team usually started by implementing the functional logic that fulfills the design. We used small volumes of data to validate the functionality. During these functional development tasks, there are a few things that deserve our additional attention because they could impact the system’s stability when the system is running in full scale and handles production workload during peak time. . Cloud is a live infrastructure: Cloud is an environment that keeps evolving over time. All cloud vendors are working hard to improve existing services and introduce new capabilities in their platform. It also means the services on the cloud will be upgraded every couple of months. Though cloud vendors/operators will try to prevent service interruptions and sometimes reduce the interruption time to less than one second, it still impacts heavy load systems/components that happen to have hundreds of data processing tasks at the moment. Some design pattern like Claim-Check Pattern or Queue-Based Load Leveling pattern could help mitigate the impact. | Network Transient Failure: Network is the vessel of a modern system. It connects every component and transmits a massive information payload in high frequency. Many services and SDKs already have build-in mechanisms to handle transient network failure. However, for a system with heavy network traffic, we should review how it works in the architecture and consider adding additional mechanisms. The most common practice to handle network failure is to resend and retry the network operation. What needs to pay attention is it’s better to implement a “smarter” retry strategy for a system that has a huge workload and heavy network operation. Some libraries like Polly (.net core), Tenacity (python) can help us implement the retry action using an advanced algorithm. | Optimize Disk I/O: Understand the distributed file storage system underneath your data solution. For example, check Sharding for Mongo DB, Extents for Azure Data Explorer, Partitioning (bucketing) for Databricks Delta table. | Check how data indexing and partitioning impacts the disk I/O patterns. | Understand the max data access API call for your cloud file storage solution. Remember, reading one file is not an API call; it will require multiple API calls such as directory lookup, check file meta-data, read each chunk of the files. | . | Log exploding: In most systems, there will be trace logs to help system administrators monitor and debug the system’s operation status. Pay attention to how these logs are being triggered and the file size growth speed of logs. In a large system, the log files could increase from KB to multiple GBs in a few hours and drag the system’s performance. If the system uses checkpoint files, logging API calls, they also need to be checked. | . #7 End-to-End Testing . Image by Gerd Altmann from Pixabay . Testing is a vast topic. It will/should occupy at least nearly half of the developing team’s resources. Since projects always don’t have enough time to do perfect testing, we can only try to spend our limited time on the parts that most likely go wrong and is critical to the system. In data project, we realized the traditional unit test and system integration are not enough to verify the solution quality. . In our projects, dev teams need to implement the code logic that retrieves, move, clean up, process, and store data. Traditional software practices focus on unit testing and integration testing. They can verify if the code logic is correctly and thoughtfully implemented. But to ensure each developed components meet go-production quality, besides ensuring the code is implemented right, another equally important part is to test and make sure the infrastructure is also implemented right. For example, we want to check: . If the infrastructure can scale adequately to handle different data volume | If the computing resources are efficiently utilized for diverse workload | If all the tasks can be finished after peak hours | If we calculate the required system resource and make projections in the right way | If the system boundary/limitation is what we understand | . To know the answer to these questions, We adopted a few techniques: . Mimic Real World and Edge Case Workload: . The simplest way is to deploy the whole environment and run end-to-end testing using workloads that mimic the production scenario or scenario we want to test. Unlike website load testing, this part usually takes efforts to customize and can be a small standalone project on its own. To build the workload simulation solution, Kubernetes is a popular choice. It’s flexible and cost-effective. . Leverage Gherkin language and BDD Tools: . “Behavior-driven development (or BDD) is an agile software development technique that encourages collaboration between developers, QA and non-technical or business participants in a software project” - behave doc . We want to know how the system performs in different workloads and found it easier to describe the workload criteria and expected results in a structured pattern. BDD tools like Cucumber, behave, behat and their script language Gherkin (a business Readable language for behavior descriptions) match such requirements. Business users can easily change the workload patterns and automatically run the test to verify if they meet the performance requirements. . Provision Load Testing Environment using CD pipeline parameters : . The other practice is integrating the load testing environment into the CI/CD pipeline and using a parameter to control the kind of environment the CD pipeline will provide. Due to the tight integration with infrastructure and relying on some cloud PASS services, the CI/CD pipeline will become more complicated and requires careful planning. Plan and Integrate the provision of load testing environments in the CD pipeline can reduce the CD pipeline number and make it easier to keep the environment consistent. . Centralize Performance Metrics and Perpare Dashboard: . Since there are several components in the system, each part has different performance metrics, with an entirely different measuring unit and business meaning. It’s easier to have a centralized place to query and view all these metrics. . . #8 Measure your Data Drift . Data Drift is a big issue in machine learning systems. It means the profile and distribution of the data changed, and the previous trained/tested model might not be able to predict the result on the new data with the same accuracy. But not only impacts machine learning prediction, data drift will also affect the data processing pipeline. . When we design and test the data processing pipeline, it relies on understanding data to optimize the operation. Especially for data aggregation or classification operation, because most data are not evenly distributed, the unbalanced data (skew data) will introduce an uneven workload that will cause low utilization of system resources. We will need to make some adjustments to shuffle data evenly. If data drift, it will break the optimization and cause an even worse unbalanced workload and reduce system efficiency. . . Consider adding some metrics to monitor data drift if some components in the system are sensitive and will be impacted by data characteristic change. For example, we watched some Spark tasks’ running time because they are sensitive to data distribution in our scenario. The other common practice is checking the statistical distribution of recent data regularly. . #9 Continuous learning with Chaos Engineering . “Chaos engineering is the discipline of experimenting on a software system in production in order to build confidence in the system‘s capability to withstand turbulent and unexpected conditions” - [Principles of Chaos Engineering](https://principlesofchaos.org) . Chaos Engineering is initially proposed by Netflix in 2010 because they wanted a better way to test and improve their distributed microservice system’s reliability. They found that finding faults in a distributed system goes beyond the capability of standard application testing. Instead of testing the single failure point, chaos engineering aims to generate new knowledge about inter-connections and impacts between components in the system. . image credit: Chaos Engineering: How it Works, Principles, Benefits, &amp; Tools. . Improve system resilience is a continuous learning journey. Though we have mentioned several key learnings, and some of them learned from the production system issues, there are still many parts that can be explored and enhanced. We can systematically learn the system behavior through chaos engineering and understand how it responds to production turbulence. For a large data project, it is definitely worth including the chaos engineering tools and practices during system development and long-term system operation. . . This post covered topics learned from several data projects I participated in 2020. I will use it to remind me of some key points that should be considered and prepared for future data projects. Hoping some of them can also benefit your data projects. . Goodbye 2020 and Welcome 2021. . References . Best Practices for Building Robust Data Platform with Apache Spark and Delta | Analytics Challenges — Big Data Management &amp; Governance | An overview of ETL and ELT architecture | ETL Vs ELT: The Difference Is In The How | Intro to Chaos Engineering (Video) | Evolutionary Database Design -Chaos Engineering: How it Works, Principles, Benefits, &amp; Tools | .",
            "url": "https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/azure/data%20pipeline/reflections/2021/01/01/Reflection-of-my-2020-Data-Projects-part2.html",
            "relUrl": "/azure%20data%20explorer/kusto/data/azure/data%20pipeline/reflections/2021/01/01/Reflection-of-my-2020-Data-Projects-part2.html",
            "date": " • Jan 1, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Reflections on my 2020 Data Projects  - Part I",
            "content": "Lessons I learned and want to keep in mind in 2021+ . Image by Syaibatul Hamdi from Pixabay . 2020 has sunsetted. Looking back over it, I had three projects that needed to process multiple terabytes of data regularly (one of them needs to process hundreds of terabytes of data every day) and one project that wanted to apply good MLOps practices on GitHub. Some projects have gone production and are running on data centers in different continents to serve worldwide customers. . In this blog post, I want to summarize the key practices learned from these projects which I want to remember and keep in mind in 2021. I want to read them each time before a new data project kicks off and make a better plan. . Note! Even for similar workflow, small/medium/large scale projects require different architecture and plan differently. This post focus on large scale data project. . #1 Prepare for Data Schema Change . This is a hard fact, but real-world data always evolve and data schema will change overtime. Years ago, it was a common practice that we spent a lot of time negotiating the data schema/contract with data providers and tried to define the best schema that can live forever. But we are in a world that data is exponentially growing; more and more data comes from non-traditional OLTP systems. In some projects (especially projects with over several gigabytes of data every day), it’s not realistic to assume we can have a stable data format/schema over time. The requests to support varying data schema keeps pop up in system requirements in my 2020 data projects. . There are several techniques that can help to handle data schema changes, such as: . Bronze, Silver, and Gold architecture | Extract, load, transform | Using data format that supports dynamic schema such as JSON, Parquet, and leveraging capabilities of NoSQL solutions such as MongoDB, COSMOS DB, Elastic Search, Azure Data Explorer, Delta Lake, Big Query, DynamoDB… | Schema versioning. You can adopt some techniques like Temporal Patterns, Artifacts are version controlled with application code, Data Mapper, Document Versioning Pattern to implement data versioning. | . Most of the techniques share similar principles and can complement each other in handling data schema change. The key point is paying some extra attention to it. If someone assumes that they will have a stable schema, it’s better to discuss more on this assumption and validate again. The requirement to handle schema change will impact the foundation of the whole system design. . #2 Design for Query &amp; Charting . Image by David Schwarzenberg from Pixabay . A good data system should at least fulfill the team’s basic query and charting requirements. It is very common that data collectors (such as data engineers, people who collect and ingest the data) and data consumers (BI reporter, data analyzer, ML researcher.. ) are from different teams, and they don’t understand each others’ languages in projects. Considering the skill sets required for different roles and different groups also have different business goals, this is very normal. However, bringing the promised benefits of the whole data system requires deep engagement from both parties. A couple of times, we saw that technologies are chosen and built based on one side’s requirements, but the solution poorly meets the other side’s needs. . It is usually easier to figure out the data collector requirements than data consumers’ needs because data collectors’ requirements are mostly related to the 3Vs of Big Data - Volume, Variant, and Velocity. And we can quantify them with specific numbers in the requirements specification. The data consumers’ requirements often depend on the business context, problem domain, and size, skill set of the data consumers team; it requires some more digging to figure out. . Here are some common factors to consider: . Visualization Tools: Some teams may have strong Excel skills; they are fluent in using PowerPivot and Excel functions to generate in-depth interactive reports. Some teams like the advanced graphical presentation provided by dashboard/reporting tools such as PowerBI, Tableau. Some groups like the powerful programmability in Jupyter Notebook or Shinny. Also, most of the data platforms will provide their build-in tools to visualize data. For example, Azure Data Explore provides very handy and powerful charting capabilities for historical log data and time-series data analysis. Residual time series chart in Azure Data Explorer . | Data Post Processing/Query Tools and Languages: Data consumers will need to further process data to fulfill their analytic requirements. Languages like SQL, R, and Python (ok, you can also add Julia) are very common and powerful ones that data analyzers use to further “massage data”. But each analysis domain has it’s own unique data processing pattern. Based on different design purposes, different data platforms also provides their own data query syntax and query engine to support and simplify data post-processing for specific domains. Understanding each platform’s strength and choosing the best-fit one is critical to the project’s success. For example, it’s easy to implement complex text filtering and parsing in Elastic Search, but it’s hard to implement complex data join and aggregation in it. The other example is Azure Data Explorer, which focuses on historical log data analytics, provides powerful Kusto Query Lanaguage to simply log parsing, aggregation, data join, and semi-structure data processing. It is one of my favorite query languages. . | Performance goal and Limitation to retrieve insights from “Big Data”: Processing large volumes of data is not easy; different solution applies different strategies to achieve their performance goal. They use different index algorithms, in-memory computing, cluster resource allocation, file storage structure, data compression type, etc., to archive their performance goal. These strategies will also come with some limitations like “limitation of concurrent users”, “unbalance computing resource allocation”, “required huge memory caches”, “data paging support”…etc. Review the pro and cons of different solutions and test them in the project’s main data query scenarios. . | . Both data collectors and data consumers are critical stakeholders of a data project. In the solution design phase, it’s helpful to have deep engagement with both two teams. Spending a little more time on how the data query pattern looks like and considering the data analysis tools suitable for users, you will have a bright outlook for the system. . #3 Balance the workload of each resources . image credit: Rob Leane -Den of Geek UK. . When we worked on small or medium-sized data projects, we implemented the solution using one or two key technologies such as Databricks or Azure Data Explorer. These technologies come with some default capabilities to ingest, manage, and query data. We focused on providing proper configuration on these platforms. . However, when we worked on large scale projects, instead of relying on built-in capabilities, we found we need to re-architect the system to further distribute the computation workloads to different components in the system. We need to spend more time thinking about how to make the data more “swallowable” for the data platform. “Swallowable” could mean things like reducing the data fragmentation, easier to build index, remove dirty data, remove duplicated data… etc. . The workload distribution process is also a process to find the balance of each component. Here are some points that might help: . Size of your batch and how it impacts latency and memory consumption | Choose different data formats, consider it’s the impact to data serialize/un-serialize time and disk I/O | Decide where to filter, split, aggregate data so the next component can reduce workload and simplify management and configuration complexity. | It is also related to the maximum scale-out/in capabilities of each component. All the data input and output velocity of these components can then be within its connected components’ bandwidth. | . For a large scale project, the right solution will be the tasks are well distributed among all critical components, and the workload is balanced. So the designed system can have room growth its capacity, handle peak-time traffic, and prepare for future business growth. . #4 Wow, there are duplicated data . This is another common issue when we process a large amount of data. We could have duplicated data for dozens of reasons. It could be the data source sent twice because of a network issue; it could be some parts of the data processing pipeline partially failed and the system is trying to resume from the latest checkpoint; it could be the underneath message service only guarantee at-least-once, etc. . Image by Martin Pyško from Pixabay . We need to evaluate the business impact and the cost we want to pay for mitigating the issue. Common strategies to handle duplicated data are: . Ingestion Time Check: We can have ingestion time check by adding data processing logs or checkpoint files. Then all the ingestion operations need to verify the log/checkpoint file before process the data. | Settling Basin Filter: In this strategy, we store data in a temporary storage/table. Then, we compared the data with recently ingested data to make sure they are not duplicated data. | Query Time Check: In some systems, a few duplicated data can be ignored in most use cases (e.g., website user login log for analyzing user demographic/ geographical distribution), and only a few of its use cases require duplicated data check. For such a scenario, we can check duplicated data at query time for only required use cases; then we can reduce the system cost for de-duplicated data check. | Under Some Threshold, Ignore It: It’s the best solution if the business context allows it. De-duplication operations usually occupy a certain amount of system resources and impact system performance. In a large data processing system, it’s very costly to implement it. It will be helpful if we can ignore it when it doesn’t impact the business goad. | . Besides the above strategies to handle duplicated data, one other point is understanding where the duplicated data comes from. Which part of the system or infrastructure services provides at-least-once and which part can support exactly-once. If the system use checkpoint to resume the failed operation, try to understand how it works. . #5 Understanding of Core Technologies . Image by Gerd Altmann from Pixabay . This point should be needless to say, but I add it here not just because it’s fundamental but also it could take consider mount of time to understand the design philosophy and available configuration parameters of the different data platforms. Many settings could impact each other; we should keep in mind that planning enough time to learn, test, and verify the best configuration setting is required. . For example, in the core technologies I used in these projects, here are some key configurations that need to check, calculate, and test: . Databricks Number of Cores | Number of Memory | Duration of Job, Stage, Tasks | Fair scheduler Pool | Structured Steaming : max-files, max-size per trigger | Shuffle Partitions | Shuffle Spill (memory) | . | Azure Data Explorer Ingestion MaximumBatchingTimeSpan | MaximumNumberOfItems | MaximumRawDataSizeMB | . | Capacity Policy Ingestion capacity | Extents merge capacity | Extents purge rebuild capacity | Export capacity | Extents partition capacity | Materialized views capacity policy | . | . | Azure Functions (Python) FUNCTIONS_WORKER_PROCESS_COUNT | Max instances when Functions scale out | Queue Trigger batchSize | newBatchThreshold | . | HTTP Trigger maxConcurrentRequests | . | . | . » Part-II . References . Best Practices for Building Robust Data Platform with Apache Spark and Delta | Analytics Challenges — Big Data Management &amp; Governance | An overview of ETL and ELT architecture | ETL Vs ELT: The Difference Is In The How | Intro to Chaos Engineering (Video) | Evolutionary Database Design -Chaos Engineering: How it Works, Principles, Benefits, &amp; Tools | .",
            "url": "https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/azure/data%20pipeline/reflections/2021/01/01/Reflection-of-my-2020-Data-Projects-part1.html",
            "relUrl": "/azure%20data%20explorer/kusto/data/azure/data%20pipeline/reflections/2021/01/01/Reflection-of-my-2020-Data-Projects-part1.html",
            "date": " • Jan 1, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Lessons learned from building a large scale historical data analysis system using Azure Data Explorer - Part 2",
            "content": "[This is the part II of the topic, you can check Part I if you haven’t.] . I have mentioned 5 basic lessons I learned from my recent project that has hundreds of terabytes data. In this part II I would like to share a few more advanced lessons. . Lesson 6 Consider guiding users to submit the right queries and think about having some data process running on the client-side . Azure Data Explorer(ADX) is designed to process a huge amount of data. It can efficiently handle hundreds of terabytes data or even petabytes data. Data is a very valuable asset especially when you have a lot of them. These data can support users to find hidden business trends, detect anomaly events, identify correlation things from these data. . However with so much data in your system’s back yard, you also need to make sure users will access them in the right way because users might not have a good sense of how much data they are trying to process. So we should guide and prevent users from unconsciously trying to retrieve 100 gigabytes of data to their web browser or trying to get 1 million records to their client just for showing a line chart. Helping users to use aggregation and return data in just enough granularity for their business needs can largely reduce system workload and improve performance. . Also you should consider using the user’s client to share some data process loading of ADX. For example, users might want to sort the result by some specific columns, this can be easily implemented on the client-side. It’s also easier to do data paging in client slides then resubmit queries for each page. . In ADX it provides some mechanisms such as result set size, request execution time to prevent system resource been occupied by some bad queries. You can also use these mechanisms to prevent some queries to take too many resources from the system or grant more resources to some special queries. . Lesson 7 Check some advance table policy . To further provide better query performance, ADX now provides some new more advanced configuration in Table policies such as Data partitioning policy and RowOrder policy. If you need to improve query response time, they are options you can try. But using them carefully, they will increase data ingestion time and consume more computing resources. Also, partition has some side effect on extents management and need more resources to run the policy. . The other consideration is the choice between putting data in different tables or different databases. This choice will impact the following areas in system design: . Data Access Control | Admin node workload | Data sharing mechanism | . Users should consider putting data in different databases when you want to have more flexibility to manage and share data. Also when the data volume is big, having multiple databases can enjoy the benefit of having different Admin nodes for each database and then have more resources to maintaining data metadata and perform transactions. . Lesson 8 Check Server loading and very very carefully tuning some of them if needed. . To handle data ingestion and query workload, ADX has several mechanisms to balance the resources used to support different operation and maintenance workload. . For data management operations, you can use .show capacity to quickly show the resources utilization of these core capabilities. . . In most cases the official recommendation is just leave the setting as is, the default setting should be able to handle most of the situations and they will keep each resource utilization balanced. If in some cases if you thought you really need to change these values, it’s also recommended to at least check and ask in stackoverflow to have some experts to double verify it. . Also you might also want to check the Extents number and its growth in your system. Extents are the core data allocation unit within ADX, ADX will try to organize it in the best way to fulfill both data ingestion needs and data query needs. If for some reason too much extends are created, it will increases the loading of ADX admin node and potential will impact system performance. For a large system it’s a good practice to keep an eye on it. . . Lesson 9 Optimize the KQL query plan for the long queries and review cache policy . Though we mentioned how different query syntax could impact query performance in Understand and test KQL query performance session of Part I, here I want to talk about KQL query plan. . In Kusto Explorer, there is a build-in tool named Query Analyzer which provides extra information on how your KQL runs. You can use them to understand how the query actually being executed, if your query can run in parallel on all the nodes, if the query filter efficiently reduces data retrieval volumes or if the data join. . . . Also by design initially (before the cluster is running) all the data are stored on cold storage (Azure Blob storage). When the cluster starts, cluster nodes will load all these hot data (defined by Cache policy) data into local SSD and memory so they become hot data and can be quickly accessed. Review the cache hit rate of popular queries and check if the cache policy is properly defined can potentially have a good boost of performance. . Lesson 10 Review Security Setting . Last but not least, Security. ADX can use both Azure AD and Microsoft Account (MSAs) as the identity providers. And we also need to have a plan for user authentication and application authentication. . There are different security roles in ADX. It’s good to review the security setting in your system and make sure you have separation of concerns for your account and security planning. . . Azure Data Explorer is a very powerful tool for analyzing historical log or telemetries data. In many cases we saw it provide a more cost-effective and more powerful query tool to fulfill users’ business needs. . Above are 10 lessons I thought are important when implementing a large scale data analysis platform when using Azure Data Explorer. The best way to understand the detail of them is still read through the official documents. But these lessons might provide you a quick bird’s-eye view of what you should check and look. . [Reference] . - kusto-high-scale-ingestion . - Azure Data Explorer technical white paper . - Update policies for in-place ETL in Kusto (Azure Data Explorer) .",
            "url": "https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/kql/azure/2020/05/25/Lession-Learn-LargeScale-ADX-part2.html",
            "relUrl": "/azure%20data%20explorer/kusto/data/kql/azure/2020/05/25/Lession-Learn-LargeScale-ADX-part2.html",
            "date": " • May 25, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Lessons learned from building a large scale historical data analysis system using Azure Data Explorer - Part 1",
            "content": "Recently I had an opportunity to participate in another data project that also uses Azure Data Explorer(ADX) as the core data process and store engine. In this project we used ADX to ingest and process more than half of petabytes data. Like most projects we were under some time and resource constraints, and also encountered a few unexpected technical challenges due to the constraints. Though we couldn’t implement the system using the best-optimized architecture (it will take too much time than the project was allowed), we still managed to achieve the project goal. It’s an exciting and fun journey and here are a few lessons we learned. . Lesson 1 Select proper SKU . There are couples different SKU options for ADX, some are more CPU optimized like D-Series (D-series, Ds-series) VM, they have more powerful CPU; some are more storage optimized like Ls-Serious VM, they are equipped with larger SSD to achieve more I/O performance. Have a testing plan to test the key user query patterns on these different type of VMs and check which one is best for your query workload can benefit the project in the long run. . . Lesson 2 Check different ingestion options. . In Azure Data Explorer, it supports several different ingestion solutions, the decision will depend on the purpose and stage of your development. You can check here for detail information of these solutions. . It’s better to read through the official document, understand their differences before making a decision. Meanwhile, here are a few thumb rules: . For query testings, verifying scripts, tables, you can use Inline ingestion (push) | For ad-hoc feature engineering, data cleaning, you can use Ingest from query | For ingestion testing, create some volumes of data, you can use Ingest from storage (pull) | For production ingestion pipeline testing, I normally will use Queued ingestion. | . In addition to the above basic ingestion options, you can also check the following options based on your scenario and environment. . Ingest from storage using Event Grid subscription | Ingest from Event Hub | Ingest from IoT Hub | . And there is a new ingestion option One-Click Ingestion which are just been announced in Microsoft Build 2020. . Eeek, a lot of choices. :p . Lesson 3 Have a clean ingestion pipeline . Normally when trying to ingest data into a data repository, we might need to do some data pre-process such as check file format, clean dirty data, do a few data transformation, etc. Azure Data Explorer provides some of these capabilities through User-defined function and update policy. You can use these mechanisms to quickly perform some data pre-process tasks within ADX without setup extra computing services to handle it. . While these are convenient ways to massage data, it will occupy ADX’s resources and potentially introduce more data fragmentation. There are complex mechanisms within ADX to optimize the resources it has, maintains and organize data in its storage and keep the system in a healthy status. . Under the condition that data ingestion volume is big, these data pipeline activities could impact the resource available for ADX to handle queries or do internal housekeeping tasks. You might need to carefully monitor ADX status and do a few fine tunes on its configuration. While I did use these mechanisms for other smaller-scale projects and love it, it is still a better practice just to keep the data pre-process tasks outside of ADX in large scale project, at least before you are very familiar each internal mechanisms within ADX.) . Data Factory, Databricks, Azure Functions/App services, AKS, HDInsight provide good foundational capabilities to pre-process data. . . In my git project I shared some codes that I used Azure Functions to do Queued ingestion. . Lesson 4 Evaluate the horizontal scale (# of servers) needed . When planning system roll-out, a solid estimation of the number of servers needed and a well-estimated expansion plan for the future is important. It can also help save costs by preventing under-utilization and provide valuable information for system design. System scale out/ horizontal scaling is one of the core capabilities that we can make sure the system can be adaptive to the workload and provide just enough resource to the users. In our test, one of the key ADX features that users love is it can provide almost linear performance growth when scaling out. ADX also provides non-destructive services when scaling out. . . It’s suggested that you should run a few workload simulations and test about how the scale-out can increase your system capabilities. . . ACI and AKS can be helpful if you want to simulate the system workload. . Lesson 5 Understand and test KQL query performance . One of the key strengths of ADX is its powerful query language Kusto Query Language (KQL). Like most other data query languages, some query operators in KQL are similar, give you the same result but could have a huge difference in performance. . It’s always good the validate what’s your key query scenarios and try how different query syntax impact the query performance. . . You can review the query performance using Kusto.Explorer tool or ADX Web UI. You can also use .show queries operator to review the performance of historical queries. . . . .. to be continued . Lessions learned from buiding a large scale historical data analysis system using Azure Data Explorer - Part 2 .",
            "url": "https://herman-wu.github.io/blogs/azure%20data%20explorer/kusto/data/kql/azure/2020/05/21/Lession-Learn-LargeScale-ADX-part1.html",
            "relUrl": "/azure%20data%20explorer/kusto/data/kql/azure/2020/05/21/Lession-Learn-LargeScale-ADX-part1.html",
            "date": " • May 21, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Troubleshooting  Azure Data Explorer (Kusto) cross-tenant access issues",
            "content": "Azure Data Explorer Introduction . Azure Data Explorer (ADX, aka Kusto) is a very powerfully log/historical data analysis platform provided by Microsoft that powers several key Azure services such as Application Insight, Azure Monitor, Time Series insight. It is designed to handle huge amounts of historical data and can ingest and process Peta-bytes of data every day with little efforts to set up the infrastructure. On February 7, 2019, Microsoft GA the service to customers. . . One of the features that I particularly like is its query language KQL (Kusto Query Language), KQL combines the concept of SQL query and data pipeline. In real project experience, it is very powerful and saved me a lot of time to get the query result in the structure that my projects need. You can check this official tutorial that introduces KQL’s key concept and following is an example about how to query data 12 days ago and do count aggregation very 30 minutes. It’s a very popular bin count pattern when analyzing data on time dimension. In the query we use “mv-expand” operator to make sure there is still a record presents the 0 count even the system has no data in that 30 minutes range. “mv-expand” is also very useful when you are parsing and expanding JSON data. . let StartTime=ago(12d); let StopTime=ago(11d); T | where Timestamp &gt; StartTime and Timestamp &lt;= StopTime | summarize Count=count() by bin(Timestamp, 30m) | union ( // 1 range x from 1 to 1 step 1 // 2 | mv-expand Timestamp=range(StartTime, StopTime, 30m) to typeof(datetime) // 3 | extend Count=0 // 4 ) | summarize Count=sum(Count) by bin(Timestamp, 30m) // 5 . . You can also easily generate a graphic chart to visualize your query result through “render” operator. In the previous query, you can add “| render timechar” to generate the following graph which represents the trend of how many records we have every 30 mins eleven days ago. . |render timechart . . For data consumers ADX also provides various client tool that users can use to create different kind of query, graphic chart and dashboard. The tool that support ADX includes : . ADX WebUI | Kusto Desktop Client tool | PowerBI | Excel | Jupyter Notebook(using KQLMagic extension) | Grafana | Spark . | . Azure Data Explorer Access Control . In the access control part, ADX supports user authentication through Microsoft Accounts (MSAs) and Azure AD account. Microsoft Account is non-organizational user accounts, these accounts normally use email like hotmail.com, live.com, outlook.com. Azure AD account is created through Azure AD or another Microsoft cloud service such as Office 365. It will be tight to an Azure AD tenant and is the preferred method for authenticating to ADX. Most enterprise users should use AAD account for authentication. Here has more information about ADX access control. . Like most Azure services, you can manage user accounts and their access to ADX through “Access control” function within Azure Portal. . . The other way to manage it is through KQL query. You can run the following command to check the accounts and roles that can access ADX database. . .show database [Database Name]] principals . You can read ADX Security roles management session to understand more about using KQL to manage user access control. . ADX authentication is also the part I would like to share a few troubleshooting tips which I learned from projects. . Azuer Data Explorer cross-tenant access issue . Ideally if you login your PC with Azure AD account that in the same tenant of the Azure subscription which been used to create the ADX cluster, then everything should work good and you can just management user access in Azure Portal. However it’s not always the case, users can come from different organizations and partners. Here are a few ways we used to check and fix the issues. . The problem: . Grant a user to access ADX in Azure Portal UI. And the user can assess ADX in Azure portal. But he/she but couldn’t access it in ADX Web UI and Kusto Explorer, PowerBI, Excel or other client tools. . What happened is when you grant a new user to access ADX, if the user’s account comes from a different tenant, in ADX it will create a new Principle Object Id in its tenant. If the user access ADX in Azure portal, the user’s account already switches to the right tenant so there is no problem accessing ADX. But if users from different tenant try to use other client tools, these client tools will use default tenant and which might not match the tenant that ADX is using. . How to fix the issue . Solution A: Force ADX WebUI and Kusto Explorer to use not ADX’s tenant . ADX WebUI . Default ADX WebUI has URL like . https://dataexplorer.azure.com/clusters/[cluster name].[data center] . You can force it to recognize a user’s login tenant by add &amp;tenant=[tenant id] to the URL. So it will look like . https://dataexplorer.azure.com/clusters/[cluster name].[data center]?tenant=[tenant id] . | Kusto Explorer Client . Instead of default connection by specifying ADK cluster URL, You need to use advanced query. . | . . The connection string will be like: . Data Source=https://[Cluster Name].[Data Center].kusto.windows.net;AAD Federated Security=True;AAD User ID=[User&#39;s AAD account(E-mail)];Authority Id=[User&#39;s AAD account tenant ID ] . . Solution B: Grant user using ‘right’ tenant id and account id . As said by default user granted in Azure portal will using ADX’s tenant id and create a new account object id. If you want to add a user from a different tenant, the suggested way will be to grant users using KQL. . You can grant a new user with different roles like . .add database [Database Name] [Role Name] (&#39;aaduser=[User AAD account id];[User AAD tenant id]&#39;) &#39;Notes for the account, eg. User Name&#39; . Get AAD account object id &amp; tenant id using Azure CLI . To get user’s AAD account object id, you can use Azure CLI command : . | . az ad signed-in-user show . To get the user’s tenant ID, you can use Azure CLI command : . az account list . Get AAD account object id &amp; tenant id in Azure portal . You can also find AAD account object id &amp; tenant id in Azure Active Directory service of Azure portal. . | . . Get AAD account object id &amp; tenant id in ADX Web UI . If you login using ADX Web UI, actually the error message contains the AAD user account id and tenant id. The message will be looks like following . Error message : action Principal ‘aaduser=[AAD accound id];[ AAD tenant id ]’ is not authorized to perform operation . It’s another easy way to get the information you needed. . | . ADX is a very powerful platform that provides interactive analysis capabilities which can handle huge amount of historical log data with little infrastructure maintenance efforts. Because most enterprises use Azure AD to grant users access, helping these tips can save you sometime when you have cross tenant access requirements. .",
            "url": "https://herman-wu.github.io/blogs/azure%20data%20explorer%20(kusto)/data/2020/03/17/TroubleShootingADXLoginIssues.html",
            "relUrl": "/azure%20data%20explorer%20(kusto)/data/2020/03/17/TroubleShootingADXLoginIssues.html",
            "date": " • Mar 17, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Test",
            "content": "Test Images . This is a Test . . https://github.com/Herman-Wu/blogs/blob/master/images/diagram.png?raw=true . .",
            "url": "https://herman-wu.github.io/blogs/2019/03/03/Test.html",
            "relUrl": "/2019/03/03/Test.html",
            "date": " • Mar 3, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Senior Software Engineer currently @ Microsoft . . Mostly work on Data and Machine Learning engineering. Interested in things behind the number. . Love funny projects, music, travel, hiking, ancient things, building, photograph, drone. . . . . Twitter: hermanwu01 | LinkedIn | .",
          "url": "https://herman-wu.github.io/blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}